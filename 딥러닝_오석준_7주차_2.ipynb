{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8790808e-6734-4bdd-83f6-4b4255198475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 28, 28)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randn(10, 1, 28, 28)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85bd316b-c923-4bbd-a61b-b03508875026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7873835f-2f3b-4f6e-8e51-aeb97d6bba03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c794802d-1ab4-4331-a668-35b1baf34819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.64728621e-01, -7.51978664e-01,  1.62427887e+00,\n",
       "         3.47114022e-01,  1.56944838e-01, -1.27468304e+00,\n",
       "         1.16884674e-01, -1.01647176e+00, -6.69748988e-02,\n",
       "        -1.48213493e+00,  7.48471102e-01, -3.57560650e-01,\n",
       "        -3.13745364e-01, -2.26625816e-01, -1.04910653e-01,\n",
       "         8.15905769e-01,  2.02040913e-01,  9.16141810e-01,\n",
       "        -8.23548666e-01, -1.69189145e+00,  1.96698916e+00,\n",
       "         6.79130575e-01, -3.50853706e-01, -9.19325704e-01,\n",
       "         1.42429208e+00,  3.89638617e-01,  2.78017235e-01,\n",
       "         2.75082644e-01],\n",
       "       [ 6.44715965e-01,  1.02210747e+00, -6.83771595e-01,\n",
       "         6.64526617e-01, -7.06986859e-01,  8.29532010e-02,\n",
       "         1.46312044e+00,  1.35326283e+00,  4.04558358e-02,\n",
       "         7.89808621e-01, -4.90338295e-01,  4.12504575e-01,\n",
       "         1.99813675e-02, -3.06733845e-02,  1.98097864e-01,\n",
       "         5.81690057e-01,  1.38271387e-01, -1.70479567e+00,\n",
       "        -1.84752267e-01, -1.29838805e+00,  2.31681825e-01,\n",
       "         4.84538636e-01, -1.46977583e+00, -5.29789272e-01,\n",
       "        -1.30514331e+00, -5.35921432e-01,  6.62142749e-02,\n",
       "        -1.94478351e-01],\n",
       "       [ 3.67482355e-01, -5.24266071e-01,  4.78057497e-01,\n",
       "        -6.93372663e-01,  2.38771767e+00, -1.82288380e-01,\n",
       "        -5.57406436e-01, -5.61870335e-01,  3.40527478e-01,\n",
       "         9.17314517e-02,  7.11932371e-02,  9.83928544e-01,\n",
       "         1.26004726e+00,  1.46296952e-01,  6.04228519e-01,\n",
       "        -1.29667767e+00, -2.01383699e-01, -2.00070808e-01,\n",
       "        -1.77736543e+00, -6.59686218e-01, -1.38473281e-01,\n",
       "        -2.01245740e-01,  4.31450129e-01, -7.15339389e-01,\n",
       "         2.40724416e+00, -3.80431696e-01,  1.64693335e+00,\n",
       "        -9.34780142e-01],\n",
       "       [-4.77205844e-01,  9.79726485e-01,  2.08615802e+00,\n",
       "        -2.33291129e-01, -5.66588058e-01, -1.01423799e+00,\n",
       "         1.94154752e+00,  4.90563951e-01,  6.91564923e-01,\n",
       "         1.24869550e+00, -1.53092694e-01,  2.06486256e+00,\n",
       "         1.13200692e+00, -6.99725985e-01, -3.76991597e-01,\n",
       "        -3.64653706e-01, -3.85034766e-02,  3.41355034e-01,\n",
       "        -2.84542583e-01,  1.27096444e+00,  1.35307967e+00,\n",
       "        -1.12598736e-01, -1.59990889e+00, -8.25253032e-01,\n",
       "         1.07424131e+00, -2.03836859e+00, -3.86077259e-01,\n",
       "         1.35528631e+00],\n",
       "       [-7.26634790e-01, -1.39672716e-01, -2.88377310e-01,\n",
       "        -8.17541530e-01, -2.09030525e-01, -6.06194460e-01,\n",
       "         2.08011318e+00,  1.36347669e+00, -8.07782363e-01,\n",
       "         1.53661657e+00, -4.06184199e-02, -2.05962106e+00,\n",
       "        -1.37455168e+00, -1.26712650e+00,  8.94079856e-02,\n",
       "         8.62346047e-02, -1.56410298e+00,  1.98039016e+00,\n",
       "         8.89623832e-02,  1.88582786e+00,  1.61890614e-01,\n",
       "        -8.06038324e-01, -5.95244792e-01, -3.34660608e-01,\n",
       "        -1.17586151e-01,  1.62925509e+00, -1.39295454e+00,\n",
       "        -1.58170919e+00],\n",
       "       [ 2.95292446e-01,  1.32450032e+00, -2.79571011e-01,\n",
       "        -5.03918333e-01, -1.57872299e-01,  1.11132426e-01,\n",
       "        -7.71389307e-01, -1.23665571e+00, -5.50402459e-01,\n",
       "        -3.38084610e-01, -1.32025162e+00, -5.61391355e-01,\n",
       "        -4.38405656e-01,  6.42826138e-01, -1.57282935e-01,\n",
       "         2.15797352e+00, -7.30805066e-01, -2.56290230e-01,\n",
       "        -6.33610198e-01,  5.39036705e-01, -1.30167236e+00,\n",
       "         4.33664789e-01,  7.32382804e-01,  7.24778789e-01,\n",
       "         6.45431935e-01,  3.05491939e-01, -1.77367836e+00,\n",
       "        -1.14988498e+00],\n",
       "       [ 2.42230956e-01,  9.20172709e-01,  1.17076084e+00,\n",
       "        -2.00587986e+00, -1.38639904e+00,  8.93364784e-02,\n",
       "        -2.93112147e-01, -8.20267222e-01, -2.64815895e-01,\n",
       "         2.61711401e-01, -8.14455268e-01, -5.95840178e-01,\n",
       "        -5.99137479e-01,  1.10809681e+00, -2.12031694e+00,\n",
       "         6.26819967e-01, -1.93421399e+00, -1.46249004e+00,\n",
       "        -4.61734413e-02,  1.67056396e+00, -4.27725615e-02,\n",
       "        -5.65025892e-01,  1.77991224e+00, -1.88368092e+00,\n",
       "         1.01880310e+00, -9.61595375e-01,  9.47482482e-01,\n",
       "         9.94886697e-01],\n",
       "       [-6.12529321e-01,  1.24960756e+00, -4.84596643e-01,\n",
       "        -5.42157342e-01,  1.39129778e-01,  9.69515009e-01,\n",
       "        -8.42719275e-01,  1.19233273e+00,  6.58931736e-01,\n",
       "         9.34751218e-01, -1.83430600e+00,  9.23296229e-01,\n",
       "         3.72839628e-01,  1.26397526e-01,  1.48246677e-01,\n",
       "         7.30289470e-02, -5.18718304e-01,  8.19757466e-02,\n",
       "         6.67343860e-01, -2.10384873e+00,  1.54660518e+00,\n",
       "         2.97962657e+00, -1.31465911e+00,  6.04017326e-01,\n",
       "        -1.24966212e+00,  1.28965375e+00,  1.74442248e+00,\n",
       "         3.52664908e-01],\n",
       "       [ 1.10096389e+00, -2.58368534e+00,  2.77437100e-01,\n",
       "        -2.66200804e+00,  6.69708893e-01,  1.45884705e+00,\n",
       "        -1.02989068e-01, -8.10339074e-01, -1.74534863e-01,\n",
       "         1.31004148e+00, -1.39989027e+00, -8.41343715e-02,\n",
       "        -1.56413383e+00,  1.00581424e+00, -1.16017723e+00,\n",
       "         1.82276302e+00, -7.10282291e-01, -1.04515484e+00,\n",
       "        -1.47621626e+00,  1.76605328e+00, -7.36006119e-01,\n",
       "        -5.89897164e-01,  1.86834693e+00, -8.85814554e-01,\n",
       "         2.95181143e+00,  3.04133146e-01, -7.16373226e-01,\n",
       "        -3.09217251e-01],\n",
       "       [-1.70037235e+00, -2.76816419e-01,  8.57271903e-03,\n",
       "        -4.97038133e-01,  6.82159280e-02,  4.04391133e-01,\n",
       "         1.20928829e+00,  1.94965142e+00, -7.64603771e-01,\n",
       "         2.27218348e+00, -2.98458728e-02,  1.93828974e+00,\n",
       "         1.10736131e+00,  1.85492049e+00,  1.12554728e+00,\n",
       "        -5.44206712e-01, -1.08811129e+00,  5.21673032e-01,\n",
       "        -1.34699771e+00, -7.36800003e-01,  3.96548488e-01,\n",
       "        -6.51104430e-01,  9.37363325e-01,  1.13137567e+00,\n",
       "         6.94520261e-01, -1.20965055e+00,  1.06643685e+00,\n",
       "         9.74542284e-01],\n",
       "       [-1.58710425e+00,  8.89546654e-01,  2.76317020e-01,\n",
       "        -1.91854423e+00,  1.87956339e+00, -8.37451229e-01,\n",
       "        -2.51597798e-01, -1.31141595e+00, -2.71918956e-01,\n",
       "        -1.04296554e+00,  1.94514643e+00,  7.42887189e-02,\n",
       "         2.39176183e-01,  8.43967203e-01,  8.71558853e-01,\n",
       "         1.34132290e+00,  6.41575249e-01, -2.03451964e+00,\n",
       "         1.98046792e+00,  3.56390280e-02, -7.23192356e-01,\n",
       "        -4.88519484e-01,  2.23812420e+00,  4.45439628e-01,\n",
       "        -1.08008767e+00, -4.45742435e-01, -1.88228498e+00,\n",
       "         1.01709225e+00],\n",
       "       [-4.02319638e-01, -1.32567087e+00,  3.49919844e-01,\n",
       "        -2.89063727e-01, -1.56945508e+00, -9.90258496e-03,\n",
       "        -9.86663222e-01,  8.62380452e-01,  2.51614960e+00,\n",
       "        -1.77065426e-01,  1.07239992e+00,  1.27326556e+00,\n",
       "        -1.31046013e+00, -7.32814001e-01,  1.79213028e+00,\n",
       "         8.84984625e-01, -1.53271482e+00, -2.90904120e-01,\n",
       "         1.30224378e+00,  1.48892074e+00,  1.11705847e-01,\n",
       "        -4.15179239e-01, -3.68762449e-01,  6.64766599e-02,\n",
       "         1.07372843e+00,  5.15371208e-01, -8.07345334e-01,\n",
       "         6.07744323e-01],\n",
       "       [ 7.05626014e-01,  7.22120355e-01, -2.34555219e-01,\n",
       "         1.62203655e+00,  1.99356132e+00, -1.30344182e+00,\n",
       "         7.91145647e-01,  9.59803743e-01,  7.85732972e-01,\n",
       "        -4.82889881e-01, -5.03616746e-01, -4.30386113e-01,\n",
       "        -1.16248479e+00, -1.12549690e+00,  7.62535344e-01,\n",
       "         5.34772791e-01, -5.71747302e-01, -8.88072911e-01,\n",
       "        -2.05462615e-01, -1.00795083e+00,  1.60382157e-01,\n",
       "        -9.16279133e-02, -1.06017843e+00,  2.91867346e-02,\n",
       "        -1.42761550e+00,  7.11881994e-04, -8.40006786e-01,\n",
       "         9.95073544e-01],\n",
       "       [-1.38272652e-01,  9.28471118e-01,  5.39295995e-01,\n",
       "        -1.54428168e+00, -5.70066847e-01, -1.70809073e+00,\n",
       "        -1.17313315e-01, -6.22339877e-01, -6.87286486e-01,\n",
       "        -1.59264744e-01, -3.97276901e-01,  1.07769881e+00,\n",
       "        -2.23380970e-02,  3.78432219e-02,  2.37728913e+00,\n",
       "         5.10606473e-01,  2.77242033e-01, -1.17937483e+00,\n",
       "         1.32617510e+00, -1.34097479e+00, -9.90835652e-01,\n",
       "        -6.22227978e-02, -2.71641398e+00, -1.09234263e+00,\n",
       "         9.04163682e-02, -2.52807465e-01, -1.02388267e+00,\n",
       "         1.01505694e+00],\n",
       "       [ 8.58051013e-01, -1.11063519e+00, -1.83419295e+00,\n",
       "         1.41601993e+00,  1.70243336e+00,  5.02413565e-01,\n",
       "         1.44746186e+00,  1.05503230e+00,  5.72365986e-01,\n",
       "        -4.63573154e-01, -7.29680834e-01,  1.02687408e-01,\n",
       "         9.81700319e-01, -1.32730235e+00, -1.13736181e-01,\n",
       "        -5.71152669e-01,  1.21487986e-01, -2.20800787e+00,\n",
       "        -8.19007956e-01,  1.11381319e+00, -7.03683196e-01,\n",
       "        -5.79113152e-01,  1.87539292e+00,  5.75154033e-01,\n",
       "        -1.97255037e+00, -3.94651602e-01,  8.46685539e-01,\n",
       "         7.61070180e-01],\n",
       "       [ 2.99777377e-01,  1.49578134e-01, -2.12041295e-01,\n",
       "        -9.35728277e-01,  1.29821659e+00, -2.61214877e-01,\n",
       "        -1.02935213e+00,  8.45882413e-02, -3.37558387e-01,\n",
       "         1.67688566e+00,  6.42172877e-01, -4.74748077e-01,\n",
       "        -7.86979644e-02,  1.37905206e-01,  1.63261466e-01,\n",
       "         9.90445185e-01,  4.47840282e-01, -2.26702257e+00,\n",
       "        -1.87757878e+00,  4.42427526e-01, -4.51312585e-01,\n",
       "         4.97070478e-01, -8.43294891e-02, -5.53481380e-01,\n",
       "        -1.03599891e+00, -1.01055974e-01,  6.37525107e-01,\n",
       "         1.96765804e+00],\n",
       "       [ 1.42857280e+00,  1.98034698e-01, -9.62436197e-01,\n",
       "         2.19817384e-01,  3.29798847e-01, -1.55971052e-01,\n",
       "        -6.10048923e-01, -2.17519905e+00,  2.10720390e+00,\n",
       "        -4.11960612e-01,  1.27494461e+00,  1.40396616e-01,\n",
       "         1.44850346e+00, -2.22794073e-01,  1.01323051e+00,\n",
       "         1.64244915e-01,  6.81411811e-01,  1.24845968e+00,\n",
       "        -1.09584968e-01,  5.12370028e-01, -6.73488746e-01,\n",
       "        -1.24403382e+00,  4.92906878e-01,  7.48826781e-01,\n",
       "         1.61115969e+00,  7.17398449e-01,  1.84498391e+00,\n",
       "         1.17043821e+00],\n",
       "       [-1.01330707e+00, -2.45172044e+00, -4.94147424e-01,\n",
       "        -2.99064498e-01,  1.81163599e+00, -3.43247819e-02,\n",
       "         1.22657581e+00,  1.66390043e+00,  1.33702331e-01,\n",
       "         2.56200398e-01, -4.24452516e-01,  2.88141906e-01,\n",
       "        -1.68419391e+00, -3.68975246e-01,  1.84636169e+00,\n",
       "        -2.04436280e+00, -1.13018368e+00, -1.40887322e+00,\n",
       "         2.47688020e-03, -4.99907669e-01,  2.90378096e+00,\n",
       "         3.35098078e-02,  1.98654686e-01, -3.79614547e-01,\n",
       "         1.41413006e+00,  1.06297174e-01, -1.11067892e+00,\n",
       "        -1.76763050e+00],\n",
       "       [ 4.90341582e-01, -2.13086594e+00,  4.96780484e-01,\n",
       "         8.90867598e-01, -3.32182606e-01,  1.29001714e+00,\n",
       "        -8.32236473e-01,  5.30004253e-01, -2.42522386e+00,\n",
       "         1.03743424e+00, -9.79668003e-01, -8.84033296e-01,\n",
       "        -1.61587986e-02,  7.94136929e-02,  1.23940075e+00,\n",
       "         9.96822702e-01,  1.66333257e+00, -7.89622724e-01,\n",
       "         4.35284011e-01,  8.58660556e-01,  1.39292755e+00,\n",
       "        -4.71802209e-01, -2.92317160e-01, -7.17412654e-01,\n",
       "        -9.48962777e-01, -4.59279184e-01, -2.33195613e+00,\n",
       "         1.10632092e+00],\n",
       "       [-1.56912620e+00, -1.35957770e+00, -5.46408601e-01,\n",
       "        -9.92021447e-01, -1.43455832e-01,  6.50319480e-01,\n",
       "         6.17931476e-01,  8.99767436e-01,  4.87027784e-01,\n",
       "        -1.28985244e+00, -1.30359592e+00, -9.32238633e-01,\n",
       "        -2.74628853e-01, -1.75992142e+00,  4.58145206e-01,\n",
       "        -5.13782291e-01, -1.03953382e+00, -1.82052321e+00,\n",
       "         1.14606316e+00,  1.86163090e-01,  1.04084370e-01,\n",
       "        -1.61334302e+00, -1.57517130e-01, -2.52121700e-01,\n",
       "        -4.30011950e-01, -1.98018349e-01, -5.47927328e-01,\n",
       "         1.48171670e+00],\n",
       "       [-1.09839158e+00,  3.16698523e-01, -8.29064969e-01,\n",
       "         2.28519254e-01,  1.13539197e+00,  5.88222019e-02,\n",
       "        -2.68204384e-01,  5.04617543e-01,  1.51040893e-01,\n",
       "        -9.95319447e-01, -9.51006405e-02,  2.90319490e-01,\n",
       "         1.10524624e+00,  7.57604411e-01,  1.58673660e-01,\n",
       "        -1.03242283e+00, -1.70673974e-01, -1.26552326e+00,\n",
       "         7.18766884e-01,  9.04167128e-01, -3.34994201e-01,\n",
       "        -1.11130379e-01, -8.04665307e-01,  6.80377327e-01,\n",
       "         1.61466587e-01,  8.37594983e-01, -1.08359607e-01,\n",
       "         1.31813921e-01],\n",
       "       [-4.69462686e-01, -1.32001322e+00,  1.19130901e+00,\n",
       "         3.07877038e-01, -1.01100484e-01,  1.71015022e+00,\n",
       "        -1.91407120e+00, -1.87542089e+00,  7.71405049e-01,\n",
       "        -4.06089820e-01, -7.74837390e-01,  9.88248749e-02,\n",
       "        -1.31638248e-02, -9.72023037e-01, -1.28999089e+00,\n",
       "         1.58012700e-01,  2.89985176e-01, -9.68785455e-01,\n",
       "         8.57798925e-02,  2.02033315e+00,  1.76874483e+00,\n",
       "         9.70140196e-01,  5.20960050e-01,  8.04530047e-01,\n",
       "        -8.09894539e-01,  7.98987359e-01,  1.36543983e+00,\n",
       "        -2.75994359e-01],\n",
       "       [ 1.58238537e+00, -5.47176100e-01, -2.06696093e-01,\n",
       "         2.71517607e-01, -5.76530798e-01,  1.25977493e-01,\n",
       "         2.73019402e+00,  2.30649352e-01, -4.38857105e-01,\n",
       "         2.46439297e-01, -2.93239952e-01, -3.72423088e-01,\n",
       "         7.66460963e-01, -4.53409924e-01,  2.22830392e-01,\n",
       "        -1.21602498e+00, -2.26509091e-01, -8.94300361e-01,\n",
       "         8.69134025e-01, -2.05552537e+00, -1.05647966e+00,\n",
       "         1.55839339e+00, -1.32207818e+00, -3.15717171e-01,\n",
       "        -9.06488743e-02,  1.01313130e+00, -3.77330542e-01,\n",
       "         4.47587567e-01],\n",
       "       [ 5.74721448e-01, -2.48087148e+00, -3.44716300e-01,\n",
       "        -1.17240515e+00, -9.45049509e-01,  6.95601649e-01,\n",
       "         3.53923473e-01,  8.95399620e-02, -4.65854432e-01,\n",
       "         8.71115413e-01,  1.89806064e+00, -6.74184165e-01,\n",
       "         1.21168532e+00, -6.45189243e-01, -5.43109188e-01,\n",
       "        -6.83935706e-02,  4.68505594e-01, -4.58313946e-01,\n",
       "         2.98576756e-01, -3.95855248e-01, -3.83041124e-02,\n",
       "        -1.19201468e+00,  1.26774308e+00, -1.55184232e+00,\n",
       "         5.63927568e-01, -7.99537446e-01, -5.14454554e-01,\n",
       "        -1.12191933e+00],\n",
       "       [-3.25664342e-01, -7.10440577e-01, -9.09226714e-01,\n",
       "        -1.08186120e+00, -1.94857072e+00,  6.45978461e-02,\n",
       "        -1.07199251e+00,  1.88430521e-01,  1.70182133e-01,\n",
       "         7.64655161e-01, -6.57362152e-01, -2.98126946e+00,\n",
       "         1.07839201e+00, -8.38018411e-01, -1.85111104e+00,\n",
       "        -4.81182196e-01,  1.08136119e+00,  7.99832415e-01,\n",
       "         5.27242892e-01,  3.77944092e-01, -1.02260394e+00,\n",
       "        -1.24616744e+00, -1.72480801e-01, -5.18028045e-01,\n",
       "         3.97658804e-01,  6.10477120e-01, -1.19070895e+00,\n",
       "         1.58810729e+00],\n",
       "       [ 6.15187306e-01,  7.75964877e-01, -1.55162157e+00,\n",
       "        -1.51826538e-01,  5.57815704e-01,  4.38316017e-01,\n",
       "         7.20085033e-01, -9.90016267e-01, -3.90519690e-01,\n",
       "         1.02035772e+00, -1.38089943e+00,  1.35617276e+00,\n",
       "        -9.29483341e-01, -2.36749559e-01,  3.60068903e-02,\n",
       "        -8.06894886e-01, -3.96171189e-01,  6.87480491e-01,\n",
       "        -4.09594622e-01,  8.62539359e-01,  1.26987747e+00,\n",
       "        -1.79549227e-01, -3.21351519e-01, -2.77165604e-01,\n",
       "         1.99977637e-01, -1.54437614e-01, -1.44408116e-01,\n",
       "         2.13514495e-01],\n",
       "       [ 5.95465668e-01, -1.48428051e+00, -6.87873884e-01,\n",
       "        -9.28816103e-01,  6.32785425e-01,  4.93728776e-01,\n",
       "        -6.49875708e-01, -2.85193244e-01,  7.37548846e-01,\n",
       "        -7.40179363e-01, -1.33726681e+00, -2.49546848e-01,\n",
       "        -6.66666582e-01, -1.37984777e+00, -1.16175932e+00,\n",
       "         3.59479408e-01,  5.58540183e-01, -3.72584888e-01,\n",
       "        -1.16907034e+00, -1.17312128e+00, -1.63399814e-02,\n",
       "        -8.80695982e-01,  1.71503366e+00, -7.28671569e-01,\n",
       "        -1.41762530e+00, -6.40025018e-02, -8.37749169e-01,\n",
       "         6.17436814e-01],\n",
       "       [-8.18032143e-01, -1.29978207e-01, -1.30749258e+00,\n",
       "         7.97143624e-01, -1.96802204e-01,  1.30897801e+00,\n",
       "        -8.36200596e-02, -5.25978890e-01, -1.18640164e+00,\n",
       "         6.29699105e-01,  9.91570995e-01, -4.12337022e-01,\n",
       "         7.37193057e-02,  8.93949864e-01,  7.84360208e-01,\n",
       "        -7.99646876e-01,  1.30732085e+00,  5.50723993e-01,\n",
       "        -1.27277934e+00,  1.25038133e+00, -3.60659551e-01,\n",
       "        -1.17227871e-01,  4.36751467e-01, -1.34455982e-01,\n",
       "        -7.16541752e-02, -1.51971270e-01,  1.15626088e+00,\n",
       "         3.42688944e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259c0114-d213-4c41-906c-043d3e3a4c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#im2col 함수 \n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07c3cc21-2220-4553-9e51-47f4812b14f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7) #데이터수, 채널수, 높이, 너비 \n",
    "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(col1.shape)\n",
    "x2 = np.random.rand(10, 3, 7, 7) #데이터 10개 \n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f555d729-abbb-4913-aa4f-ca421d5a372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride = 1, pad = 0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1+ (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1+ (W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN,-1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f286bc75-18e6-4e59-8be3-17897d3c75e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride )\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride )\n",
    "        #전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        #최대값(2)\n",
    "        out = np.max(col, axis = 1)\n",
    "        #성형(3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88c946b4-3499-4372-ba4c-14e9fa504dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim = (1, 28, 28), \n",
    "                 conv_param = {'filter_num':30, 'filter_size': 5,\n",
    "                               'pad':0, 'stride': 1},\n",
    "                 hidden_size = 100, output_size = 10, weight_init_std = 0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2 * filter_pad) / \\\n",
    "                            filter_stride + 1\n",
    "        pool_output_size = int(filter_num + (conv_output_size/2) *\n",
    "                               (conv_output_size/2))\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                    np.random.randn(filter_num, input_dim[0],\n",
    "                                    filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                    np.random.randn(pool_output_size, \n",
    "                                    hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                    np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_params['stride'],\n",
    "                                           conv_params['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h = 2, pool_w = 2, stride = 2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],\n",
    "                                        self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],\n",
    "                                        self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dbc1d56-639e-419c-8c57-121c60400b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    for layer in self.layers.values():\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "def loss(self, x, t):\n",
    "    y = self.predict(x)\n",
    "    return self.last_layer.forward(y, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501a0ea3-947d-4aa6-b710-a4140c552394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(self, x, t):\n",
    "    #순전파 \n",
    "    self.loss(x, t)\n",
    "    #역전파\n",
    "    dout = 1\n",
    "    dout = self.last_layer.backward(dout)\n",
    "\n",
    "    layers = list(self.layers.values())\n",
    "    layers.reverse()\n",
    "    for layer in layers:\n",
    "        dout = layer.backward(dout)\n",
    "    #결과저장 \n",
    "    grads = {}\n",
    "    grads['W1'] = self.layers['Conv1'].dW\n",
    "    grads['b1'] = self.layers['Conv1'].db\n",
    "    grads['W2'] = self.layers['Affine1'].dW\n",
    "    grads['b2'] = self.layers['Affine1'].db\n",
    "    grads['W3'] = self.layers['Affine2'].dW\n",
    "    grads['b3'] = self.layers['Affine2'].db\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddd9e511-6d14-434e-942e-5814adc0ae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(os.path.dirname(__file__) + '/' + file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a272d977-ac34-470e-84af-7cd907cd4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299896562913861\n",
      "=== epoch:1, train acc:0.261, test acc:0.314 ===\n",
      "train loss:2.298274819124026\n",
      "train loss:2.2926581708615283\n",
      "train loss:2.2859140022319657\n",
      "train loss:2.2797625624652524\n",
      "train loss:2.2668834227582413\n",
      "train loss:2.2544249108881997\n",
      "train loss:2.231462270478456\n",
      "train loss:2.20960712105276\n",
      "train loss:2.1959278920964125\n",
      "train loss:2.120582348191513\n",
      "train loss:2.1033715550481915\n",
      "train loss:2.0721320053363588\n",
      "train loss:2.066461943916232\n",
      "train loss:2.0238423381223685\n",
      "train loss:1.9467572682789978\n",
      "train loss:1.9168531897204824\n",
      "train loss:1.8118325906325565\n",
      "train loss:1.664493982437444\n",
      "train loss:1.679203416696537\n",
      "train loss:1.5519250579332406\n",
      "train loss:1.379425267305507\n",
      "train loss:1.3949329621927478\n",
      "train loss:1.3630575751788603\n",
      "train loss:1.2397797112714393\n",
      "train loss:1.127827970006447\n",
      "train loss:1.0501232775018787\n",
      "train loss:1.0804683588005404\n",
      "train loss:0.9709969500077043\n",
      "train loss:0.8267444719482074\n",
      "train loss:0.8069262077125805\n",
      "train loss:0.800048620319561\n",
      "train loss:0.8160762870866666\n",
      "train loss:0.8301676107843113\n",
      "train loss:0.7004868780333695\n",
      "train loss:0.7934199182293714\n",
      "train loss:0.8267722256308175\n",
      "train loss:0.5931557656400898\n",
      "train loss:0.7794603816244506\n",
      "train loss:0.6977279176616393\n",
      "train loss:0.7259102591261466\n",
      "train loss:0.4665599400071772\n",
      "train loss:0.6819214730644155\n",
      "train loss:0.5925772272548073\n",
      "train loss:0.7355936281036931\n",
      "train loss:0.3880451073249972\n",
      "train loss:0.522414023216127\n",
      "train loss:0.55201860766885\n",
      "train loss:0.6991013590334554\n",
      "train loss:0.42212884899122616\n",
      "train loss:0.44161921136245924\n",
      "=== epoch:2, train acc:0.799, test acc:0.8 ===\n",
      "train loss:0.5883592269391821\n",
      "train loss:0.5739398049672773\n",
      "train loss:0.499280578686853\n",
      "train loss:0.6967097029104672\n",
      "train loss:0.5522746173088134\n",
      "train loss:0.5983597146733654\n",
      "train loss:0.4648506720285485\n",
      "train loss:0.3497350177960564\n",
      "train loss:0.341527315543293\n",
      "train loss:0.37095876392780985\n",
      "train loss:0.43984138714120424\n",
      "train loss:0.49323159236475583\n",
      "train loss:0.45880035425092996\n",
      "train loss:0.550486650615073\n",
      "train loss:0.5202559160886716\n",
      "train loss:0.3328459687746454\n",
      "train loss:0.27013066118539125\n",
      "train loss:0.37357956826476196\n",
      "train loss:0.324918001167479\n",
      "train loss:0.38065956343027274\n",
      "train loss:0.43364958515502655\n",
      "train loss:0.5029832040184117\n",
      "train loss:0.30172097577376583\n",
      "train loss:0.4161719030643537\n",
      "train loss:0.41387557100572203\n",
      "train loss:0.4141958775040429\n",
      "train loss:0.444107981048762\n",
      "train loss:0.42079306625985036\n",
      "train loss:0.3514258711989254\n",
      "train loss:0.41841543660039093\n",
      "train loss:0.49436370850010514\n",
      "train loss:0.3614260270499603\n",
      "train loss:0.5833482170908519\n",
      "train loss:0.3069232252406257\n",
      "train loss:0.5164600991879637\n",
      "train loss:0.2663643421667487\n",
      "train loss:0.4153847563574463\n",
      "train loss:0.352373829023496\n",
      "train loss:0.4651812881818044\n",
      "train loss:0.29495067376554734\n",
      "train loss:0.22139819655933826\n",
      "train loss:0.4429734642057752\n",
      "train loss:0.5084187888108473\n",
      "train loss:0.5378631706021609\n",
      "train loss:0.25346375050462394\n",
      "train loss:0.4502554037566357\n",
      "train loss:0.4743275991785309\n",
      "train loss:0.3259151895874783\n",
      "train loss:0.30311381466697346\n",
      "train loss:0.2894490293846579\n",
      "=== epoch:3, train acc:0.858, test acc:0.836 ===\n",
      "train loss:0.7086307955838685\n",
      "train loss:0.36848805916959476\n",
      "train loss:0.3403391578884112\n",
      "train loss:0.4145964275131147\n",
      "train loss:0.4465554311307895\n",
      "train loss:0.47869471400309976\n",
      "train loss:0.4132439915908151\n",
      "train loss:0.28467950841262446\n",
      "train loss:0.4750516600219468\n",
      "train loss:0.33675458123494534\n",
      "train loss:0.29164484530183876\n",
      "train loss:0.28397904218760867\n",
      "train loss:0.38091946291649165\n",
      "train loss:0.32001714062250564\n",
      "train loss:0.21185133050333305\n",
      "train loss:0.31967324561735416\n",
      "train loss:0.32226441387085486\n",
      "train loss:0.3731494708016615\n",
      "train loss:0.39826604667754345\n",
      "train loss:0.2714114318298055\n",
      "train loss:0.5410611023321734\n",
      "train loss:0.30259491987886766\n",
      "train loss:0.2868971133016847\n",
      "train loss:0.30961442429146807\n",
      "train loss:0.2922275999201733\n",
      "train loss:0.4198272286320225\n",
      "train loss:0.4674628063040157\n",
      "train loss:0.2867820405736578\n",
      "train loss:0.46978268809178586\n",
      "train loss:0.3711948691253309\n",
      "train loss:0.2662333250744709\n",
      "train loss:0.45308543377205773\n",
      "train loss:0.44252262833384676\n",
      "train loss:0.2811120763642248\n",
      "train loss:0.2845863574671071\n",
      "train loss:0.410150795136439\n",
      "train loss:0.38097612343148135\n",
      "train loss:0.3789894832261001\n",
      "train loss:0.3136092222121546\n",
      "train loss:0.2509648500269089\n",
      "train loss:0.41766860735077166\n",
      "train loss:0.34802648030117284\n",
      "train loss:0.3806323476044733\n",
      "train loss:0.42106269058618645\n",
      "train loss:0.22490015190378632\n",
      "train loss:0.44011643808810363\n",
      "train loss:0.20336596469618606\n",
      "train loss:0.38430641415894007\n",
      "train loss:0.24195491208913403\n",
      "train loss:0.35994437974435384\n",
      "=== epoch:4, train acc:0.895, test acc:0.89 ===\n",
      "train loss:0.23021948139769363\n",
      "train loss:0.3305105504817836\n",
      "train loss:0.36730180870409823\n",
      "train loss:0.28682059951304073\n",
      "train loss:0.26134645786865934\n",
      "train loss:0.24654047730361195\n",
      "train loss:0.2439856008108163\n",
      "train loss:0.3500486237255613\n",
      "train loss:0.2130465576677072\n",
      "train loss:0.32110161591636355\n",
      "train loss:0.2404857295141206\n",
      "train loss:0.33465903270171304\n",
      "train loss:0.2084869920691749\n",
      "train loss:0.2670432583091413\n",
      "train loss:0.20938190385963318\n",
      "train loss:0.2824690125706939\n",
      "train loss:0.2817582184328817\n",
      "train loss:0.3543884593522831\n",
      "train loss:0.22279161011038462\n",
      "train loss:0.2755549445381979\n",
      "train loss:0.1659358642603498\n",
      "train loss:0.18910054531535953\n",
      "train loss:0.2713920030013027\n",
      "train loss:0.1675402533837875\n",
      "train loss:0.24139754921660672\n",
      "train loss:0.3679426855906238\n",
      "train loss:0.20593365539155387\n",
      "train loss:0.3059790473403853\n",
      "train loss:0.3480036470227752\n",
      "train loss:0.15229874590568554\n",
      "train loss:0.2693848147832036\n",
      "train loss:0.1845250634800556\n",
      "train loss:0.3673176163633995\n",
      "train loss:0.4758050110043169\n",
      "train loss:0.186302146159078\n",
      "train loss:0.17369521974354413\n",
      "train loss:0.253840854129993\n",
      "train loss:0.25193436293725874\n",
      "train loss:0.2820720076459028\n",
      "train loss:0.17178081944800397\n",
      "train loss:0.2489257193852301\n",
      "train loss:0.2488683214024075\n",
      "train loss:0.17123573147171378\n",
      "train loss:0.26076400355179863\n",
      "train loss:0.4040976124024537\n",
      "train loss:0.15136614030531662\n",
      "train loss:0.3356590487661401\n",
      "train loss:0.18592847803587323\n",
      "train loss:0.2956016807222962\n",
      "train loss:0.2287435269342003\n",
      "=== epoch:5, train acc:0.912, test acc:0.9 ===\n",
      "train loss:0.20571810406490418\n",
      "train loss:0.22501188642239056\n",
      "train loss:0.1487531700659453\n",
      "train loss:0.2811251591927789\n",
      "train loss:0.27930655737180343\n",
      "train loss:0.19921480603294228\n",
      "train loss:0.20643340666658394\n",
      "train loss:0.24252783241108877\n",
      "train loss:0.27804540017705137\n",
      "train loss:0.24058576375068444\n",
      "train loss:0.17680932894333012\n",
      "train loss:0.24302667650978574\n",
      "train loss:0.20699317020432506\n",
      "train loss:0.2748850897690506\n",
      "train loss:0.31521343718625655\n",
      "train loss:0.25419191350107273\n",
      "train loss:0.21819005698874455\n",
      "train loss:0.3254829634958128\n",
      "train loss:0.16860385116292428\n",
      "train loss:0.14084466534725787\n",
      "train loss:0.16007677436250653\n",
      "train loss:0.23505378478256206\n",
      "train loss:0.17138446264167936\n",
      "train loss:0.14536072288840138\n",
      "train loss:0.24853543295061742\n",
      "train loss:0.32700207060551134\n",
      "train loss:0.14216059953770094\n",
      "train loss:0.19375093947415814\n",
      "train loss:0.22663540000928928\n",
      "train loss:0.2843098216465168\n",
      "train loss:0.22846403922299013\n",
      "train loss:0.15865860799844123\n",
      "train loss:0.23836394241653744\n",
      "train loss:0.14090351693591335\n",
      "train loss:0.16680711013021057\n",
      "train loss:0.1645181484443379\n",
      "train loss:0.18808171008542574\n",
      "train loss:0.1775861430123237\n",
      "train loss:0.17194922855138897\n",
      "train loss:0.19648535913770185\n",
      "train loss:0.24554969714815\n",
      "train loss:0.1857551945310944\n",
      "train loss:0.1397532904076915\n",
      "train loss:0.27567415638775167\n",
      "train loss:0.14796116411314553\n",
      "train loss:0.11580547368877184\n",
      "train loss:0.17649979895029907\n",
      "train loss:0.25445343632865575\n",
      "train loss:0.14770959307567477\n",
      "train loss:0.14180629117873994\n",
      "=== epoch:6, train acc:0.914, test acc:0.908 ===\n",
      "train loss:0.14345348987705192\n",
      "train loss:0.14726763866864287\n",
      "train loss:0.19391926286592004\n",
      "train loss:0.20206928401091628\n",
      "train loss:0.167607885274755\n",
      "train loss:0.21432254349706034\n",
      "train loss:0.2673172583600459\n",
      "train loss:0.26280376642875414\n",
      "train loss:0.3895669590810789\n",
      "train loss:0.32156118817081114\n",
      "train loss:0.24956059243056727\n",
      "train loss:0.2460545279960271\n",
      "train loss:0.10801217346126282\n",
      "train loss:0.15940134497016947\n",
      "train loss:0.23380388950703007\n",
      "train loss:0.078671507935122\n",
      "train loss:0.17715405879217141\n",
      "train loss:0.16442278639755492\n",
      "train loss:0.18883672628559506\n",
      "train loss:0.15018148404613402\n",
      "train loss:0.257801639967442\n",
      "train loss:0.11842830179964327\n",
      "train loss:0.22154023769157774\n",
      "train loss:0.16380019682985217\n",
      "train loss:0.17581259206306463\n",
      "train loss:0.19578594048320075\n",
      "train loss:0.07488109206487052\n",
      "train loss:0.20091394504735635\n",
      "train loss:0.1733644441964318\n",
      "train loss:0.16977769370338097\n",
      "train loss:0.25743826793467217\n",
      "train loss:0.20451020479290138\n",
      "train loss:0.15821052927803295\n",
      "train loss:0.22142674573525944\n",
      "train loss:0.2379514450862454\n",
      "train loss:0.12954231517967005\n",
      "train loss:0.19695961488690789\n",
      "train loss:0.2719914830235177\n",
      "train loss:0.22688929101162386\n",
      "train loss:0.1761670550585855\n",
      "train loss:0.12091758062258563\n",
      "train loss:0.18644445203879204\n",
      "train loss:0.1700534320737212\n",
      "train loss:0.14919345161806466\n",
      "train loss:0.16904276853632144\n",
      "train loss:0.15593903551786834\n",
      "train loss:0.09503045768570498\n",
      "train loss:0.1353267514979005\n",
      "train loss:0.13158037356586263\n",
      "train loss:0.11315630057509342\n",
      "=== epoch:7, train acc:0.942, test acc:0.924 ===\n",
      "train loss:0.1902852033995358\n",
      "train loss:0.12100469174148892\n",
      "train loss:0.212248064721444\n",
      "train loss:0.16625474460074371\n",
      "train loss:0.1845522371494194\n",
      "train loss:0.2504698128469908\n",
      "train loss:0.14645118059833515\n",
      "train loss:0.2019040362682215\n",
      "train loss:0.17604875124037814\n",
      "train loss:0.25178195980977597\n",
      "train loss:0.20978485904286281\n",
      "train loss:0.18663484530615357\n",
      "train loss:0.2651277565077405\n",
      "train loss:0.132240246445533\n",
      "train loss:0.21617579493977956\n",
      "train loss:0.18418407269963255\n",
      "train loss:0.11918761252000981\n",
      "train loss:0.14378444669176865\n",
      "train loss:0.20279663467856537\n",
      "train loss:0.08874158890560599\n",
      "train loss:0.17500059674096377\n",
      "train loss:0.1189877568276734\n",
      "train loss:0.20067018078114718\n",
      "train loss:0.11624045613929783\n",
      "train loss:0.22181239697088703\n",
      "train loss:0.2719377013073132\n",
      "train loss:0.15001139379462214\n",
      "train loss:0.1545541530042379\n",
      "train loss:0.10557985781777184\n",
      "train loss:0.13587611281042858\n",
      "train loss:0.26800256690448654\n",
      "train loss:0.09213189889535668\n",
      "train loss:0.17115150974212695\n",
      "train loss:0.32139204521733045\n",
      "train loss:0.03559345862508765\n",
      "train loss:0.1870545639981034\n",
      "train loss:0.07706105094877896\n",
      "train loss:0.11759724704386035\n",
      "train loss:0.1735275254465524\n",
      "train loss:0.18859148718006846\n",
      "train loss:0.07220875359745009\n",
      "train loss:0.16499313993380088\n",
      "train loss:0.24114444136972396\n",
      "train loss:0.22049466695868827\n",
      "train loss:0.1460481609997129\n",
      "train loss:0.08079726891070967\n",
      "train loss:0.20332562425470077\n",
      "train loss:0.11570152072756246\n",
      "train loss:0.12722370688414025\n",
      "train loss:0.0923772255408104\n",
      "=== epoch:8, train acc:0.955, test acc:0.925 ===\n",
      "train loss:0.054911723522248844\n",
      "train loss:0.07831761535712169\n",
      "train loss:0.23245687184779268\n",
      "train loss:0.1570643236507584\n",
      "train loss:0.10209505318022495\n",
      "train loss:0.14190589398297074\n",
      "train loss:0.07068803244379673\n",
      "train loss:0.2285107422954792\n",
      "train loss:0.18301332646971286\n",
      "train loss:0.12819498476588215\n",
      "train loss:0.08785713937777868\n",
      "train loss:0.11108899767404773\n",
      "train loss:0.20292250314129956\n",
      "train loss:0.126223454366843\n",
      "train loss:0.16523926234370548\n",
      "train loss:0.10993771071932498\n",
      "train loss:0.2224226588115741\n",
      "train loss:0.15504748158943532\n",
      "train loss:0.23536771447580673\n",
      "train loss:0.10804097575226024\n",
      "train loss:0.14560499377991562\n",
      "train loss:0.16189383994709272\n",
      "train loss:0.08322722665995537\n",
      "train loss:0.09117415606607228\n",
      "train loss:0.09993336683577901\n",
      "train loss:0.14586660123934403\n",
      "train loss:0.04151149926415847\n",
      "train loss:0.1423106118441952\n",
      "train loss:0.12960116263702764\n",
      "train loss:0.20812928097019054\n",
      "train loss:0.1069061275512926\n",
      "train loss:0.09385948301140365\n",
      "train loss:0.1599906115041236\n",
      "train loss:0.1639564095150888\n",
      "train loss:0.20165248400644237\n",
      "train loss:0.17170993142399638\n",
      "train loss:0.1323739231279356\n",
      "train loss:0.11616812492733584\n",
      "train loss:0.0802434137581453\n",
      "train loss:0.09706118146339657\n",
      "train loss:0.1892043145390935\n",
      "train loss:0.1192774053486085\n",
      "train loss:0.15163206986649902\n",
      "train loss:0.12931882729347483\n",
      "train loss:0.10552792017679687\n",
      "train loss:0.13632181197169044\n",
      "train loss:0.1782804126016821\n",
      "train loss:0.13124796777059594\n",
      "train loss:0.16199501456053766\n",
      "train loss:0.07494682352751074\n",
      "=== epoch:9, train acc:0.953, test acc:0.938 ===\n",
      "train loss:0.0731843331616437\n",
      "train loss:0.13170548725982784\n",
      "train loss:0.059101170100746885\n",
      "train loss:0.11740369115739378\n",
      "train loss:0.0785618088613563\n",
      "train loss:0.07221486718936226\n",
      "train loss:0.08178540819067052\n",
      "train loss:0.14872563890140808\n",
      "train loss:0.16027335863370612\n",
      "train loss:0.12697284262182015\n",
      "train loss:0.04850012594988442\n",
      "train loss:0.1394803249210132\n",
      "train loss:0.2149894521502834\n",
      "train loss:0.07762221151230096\n",
      "train loss:0.04776419102686752\n",
      "train loss:0.27943352921012626\n",
      "train loss:0.0770046089821833\n",
      "train loss:0.16537142512229241\n",
      "train loss:0.12805959089081775\n",
      "train loss:0.11644432272761165\n",
      "train loss:0.05407502059071164\n",
      "train loss:0.16455324703301988\n",
      "train loss:0.08169485994591044\n",
      "train loss:0.10507152936873472\n",
      "train loss:0.13325435619663697\n",
      "train loss:0.126376786910277\n",
      "train loss:0.11682476960274771\n",
      "train loss:0.14345929125386117\n",
      "train loss:0.0874376007683499\n",
      "train loss:0.08678100692204514\n",
      "train loss:0.15092855470305563\n",
      "train loss:0.12361903612718474\n",
      "train loss:0.1282568721566466\n",
      "train loss:0.0700546999980776\n",
      "train loss:0.11242787574592224\n",
      "train loss:0.09684352602189433\n",
      "train loss:0.1187560336090766\n",
      "train loss:0.06999422133682512\n",
      "train loss:0.08709811499244889\n",
      "train loss:0.0731862092753537\n",
      "train loss:0.07246741240423421\n",
      "train loss:0.09504012576887633\n",
      "train loss:0.043320776925956234\n",
      "train loss:0.04742782572000195\n",
      "train loss:0.13223883301695338\n",
      "train loss:0.07587001077150965\n",
      "train loss:0.07535630339147925\n",
      "train loss:0.11545995158670966\n",
      "train loss:0.08679432333739819\n",
      "train loss:0.19096673006334222\n",
      "=== epoch:10, train acc:0.957, test acc:0.942 ===\n",
      "train loss:0.06438035537196198\n",
      "train loss:0.13002448870628333\n",
      "train loss:0.11389896206473653\n",
      "train loss:0.10723038022213938\n",
      "train loss:0.06916238902381014\n",
      "train loss:0.10354460722950865\n",
      "train loss:0.08523779531802328\n",
      "train loss:0.043325096126242996\n",
      "train loss:0.09691929280772954\n",
      "train loss:0.1054379070315246\n",
      "train loss:0.13640660220579012\n",
      "train loss:0.08883316030165263\n",
      "train loss:0.03877003102529875\n",
      "train loss:0.21632651878387782\n",
      "train loss:0.07888457710558677\n",
      "train loss:0.060484068896998026\n",
      "train loss:0.11395203364029367\n",
      "train loss:0.15289130739705165\n",
      "train loss:0.1510967489597365\n",
      "train loss:0.07985033238668846\n",
      "train loss:0.04558289571477025\n",
      "train loss:0.04105508305847502\n",
      "train loss:0.13151001266196724\n",
      "train loss:0.0897381956427303\n",
      "train loss:0.060727995497275894\n",
      "train loss:0.09642094861858293\n",
      "train loss:0.07048325388101896\n",
      "train loss:0.13740634912049932\n",
      "train loss:0.06045946450638196\n",
      "train loss:0.138165366070695\n",
      "train loss:0.17267708411165134\n",
      "train loss:0.1251518795894295\n",
      "train loss:0.13846887504468092\n",
      "train loss:0.08047480703205233\n",
      "train loss:0.10097487544565485\n",
      "train loss:0.023878176827699094\n",
      "train loss:0.11874501014678115\n",
      "train loss:0.11890343651931515\n",
      "train loss:0.14999119043028636\n",
      "train loss:0.04622424674207532\n",
      "train loss:0.06167418863810398\n",
      "train loss:0.09027004091713672\n",
      "train loss:0.08703295395559456\n",
      "train loss:0.0766412394355014\n",
      "train loss:0.056540595587752165\n",
      "train loss:0.08174200079124215\n",
      "train loss:0.03831222398476257\n",
      "train loss:0.05955514438277709\n",
      "train loss:0.06553795971554\n",
      "train loss:0.10259189659906606\n",
      "=== epoch:11, train acc:0.967, test acc:0.947 ===\n",
      "train loss:0.10723618380481853\n",
      "train loss:0.1057054366610864\n",
      "train loss:0.14535754656800523\n",
      "train loss:0.12835570226134096\n",
      "train loss:0.05217783996611826\n",
      "train loss:0.10432714360075529\n",
      "train loss:0.058811095741052234\n",
      "train loss:0.07490209304386417\n",
      "train loss:0.03475213246101695\n",
      "train loss:0.04655820886386643\n",
      "train loss:0.11050499095735151\n",
      "train loss:0.10771998639406641\n",
      "train loss:0.07420725937087316\n",
      "train loss:0.0781698565400213\n",
      "train loss:0.07619593433948155\n",
      "train loss:0.10156978024701205\n",
      "train loss:0.061987365871132294\n",
      "train loss:0.15147869925995183\n",
      "train loss:0.09050314220516828\n",
      "train loss:0.059190929671459155\n",
      "train loss:0.09102829112842031\n",
      "train loss:0.0544345330785573\n",
      "train loss:0.0914110797161987\n",
      "train loss:0.08410447806570386\n",
      "train loss:0.07160026591447359\n",
      "train loss:0.07525059515643365\n",
      "train loss:0.024918005521635242\n",
      "train loss:0.11196122963525527\n",
      "train loss:0.08071035488655935\n",
      "train loss:0.05640757451867318\n",
      "train loss:0.05298752092556823\n",
      "train loss:0.062384746468646404\n",
      "train loss:0.039464997823388547\n",
      "train loss:0.09027946986613491\n",
      "train loss:0.14880664462305282\n",
      "train loss:0.085512422570832\n",
      "train loss:0.19229504987191828\n",
      "train loss:0.05213784909065995\n",
      "train loss:0.058836216932922694\n",
      "train loss:0.10069555767224159\n",
      "train loss:0.07603028831069544\n",
      "train loss:0.05938526516485969\n",
      "train loss:0.12467284588676635\n",
      "train loss:0.05966970295958369\n",
      "train loss:0.1357221547444626\n",
      "train loss:0.0595797120381163\n",
      "train loss:0.08715236301965107\n",
      "train loss:0.08421171459037512\n",
      "train loss:0.03018623639768988\n",
      "train loss:0.11015566645547169\n",
      "=== epoch:12, train acc:0.968, test acc:0.951 ===\n",
      "train loss:0.12510900069758127\n",
      "train loss:0.07841573571611687\n",
      "train loss:0.08504614505799507\n",
      "train loss:0.06493180231831015\n",
      "train loss:0.08434545829083899\n",
      "train loss:0.15857612805645813\n",
      "train loss:0.0742042851287268\n",
      "train loss:0.062236874755898584\n",
      "train loss:0.086146026313354\n",
      "train loss:0.12513316068265268\n",
      "train loss:0.05771131990238246\n",
      "train loss:0.031716099386232634\n",
      "train loss:0.0650343327930429\n",
      "train loss:0.07850654796536678\n",
      "train loss:0.11553759989244361\n",
      "train loss:0.07754542600288955\n",
      "train loss:0.032334950443193226\n",
      "train loss:0.05440870747610337\n",
      "train loss:0.0396674906454218\n",
      "train loss:0.014556051589832952\n",
      "train loss:0.09321197258925527\n",
      "train loss:0.18444187930671052\n",
      "train loss:0.04687978147743228\n",
      "train loss:0.07187482248182259\n",
      "train loss:0.09752401275707079\n",
      "train loss:0.08425660708723944\n",
      "train loss:0.05019743985701157\n",
      "train loss:0.16284247014697237\n",
      "train loss:0.0975290446494351\n",
      "train loss:0.08763472830225601\n",
      "train loss:0.03969956918738644\n",
      "train loss:0.15355322142693334\n",
      "train loss:0.15612572962925816\n",
      "train loss:0.096169365207059\n",
      "train loss:0.1343346583124025\n",
      "train loss:0.046847447647147586\n",
      "train loss:0.06291556642098577\n",
      "train loss:0.06697258896559756\n",
      "train loss:0.08993584941674929\n",
      "train loss:0.06875574087416407\n",
      "train loss:0.08324175176409829\n",
      "train loss:0.07839675160466097\n",
      "train loss:0.07851241773277244\n",
      "train loss:0.10014973754970112\n",
      "train loss:0.06239757948090369\n",
      "train loss:0.06451669256185487\n",
      "train loss:0.05995493061463002\n",
      "train loss:0.08692699482551566\n",
      "train loss:0.07625045095135516\n",
      "train loss:0.04182302687865429\n",
      "=== epoch:13, train acc:0.974, test acc:0.952 ===\n",
      "train loss:0.07351362627474914\n",
      "train loss:0.07354673112735992\n",
      "train loss:0.07673083638033251\n",
      "train loss:0.08334347169828565\n",
      "train loss:0.04608627044275923\n",
      "train loss:0.07964792959669303\n",
      "train loss:0.06681463896531255\n",
      "train loss:0.11423916473916725\n",
      "train loss:0.18822061074467178\n",
      "train loss:0.02923311115657051\n",
      "train loss:0.11986397942348137\n",
      "train loss:0.08180107382129204\n",
      "train loss:0.04920226777655956\n",
      "train loss:0.0544306501203807\n",
      "train loss:0.07459611399008212\n",
      "train loss:0.204751897942731\n",
      "train loss:0.11533217709601323\n",
      "train loss:0.04378091970165191\n",
      "train loss:0.10774808420874303\n",
      "train loss:0.11818386234959208\n",
      "train loss:0.06568386111386892\n",
      "train loss:0.038739249302208616\n",
      "train loss:0.0701296449630807\n",
      "train loss:0.03833867921601891\n",
      "train loss:0.058785032880972275\n",
      "train loss:0.05492209095668197\n",
      "train loss:0.05937198868577261\n",
      "train loss:0.07715319109809214\n",
      "train loss:0.09783096865252858\n",
      "train loss:0.049171842332075694\n",
      "train loss:0.03166386505258331\n",
      "train loss:0.06944325851157904\n",
      "train loss:0.11216475892743642\n",
      "train loss:0.055087177824895105\n",
      "train loss:0.05314388848382148\n",
      "train loss:0.024087166543442923\n",
      "train loss:0.02998764926767875\n",
      "train loss:0.09432249706155739\n",
      "train loss:0.07461714235014025\n",
      "train loss:0.08831838666200185\n",
      "train loss:0.07963255756021145\n",
      "train loss:0.038722876799720224\n",
      "train loss:0.027963230822562198\n",
      "train loss:0.06390974175687406\n",
      "train loss:0.05608995488429461\n",
      "train loss:0.08076748712409788\n",
      "train loss:0.04870233828629734\n",
      "train loss:0.05712330441937359\n",
      "train loss:0.0430548850924977\n",
      "train loss:0.027737796112359617\n",
      "=== epoch:14, train acc:0.976, test acc:0.956 ===\n",
      "train loss:0.028972590076075208\n",
      "train loss:0.11971572037420351\n",
      "train loss:0.0854703635337002\n",
      "train loss:0.039100184673154084\n",
      "train loss:0.022947435988734232\n",
      "train loss:0.0378165445725398\n",
      "train loss:0.040180557719151605\n",
      "train loss:0.06392056861691277\n",
      "train loss:0.09152373877751038\n",
      "train loss:0.047741806676592974\n",
      "train loss:0.03630969657239059\n",
      "train loss:0.042391709287415084\n",
      "train loss:0.06534530594034539\n",
      "train loss:0.03333832282368102\n",
      "train loss:0.05438892521058922\n",
      "train loss:0.052906816937611095\n",
      "train loss:0.04880281747656241\n",
      "train loss:0.03422824945377438\n",
      "train loss:0.08567533039312013\n",
      "train loss:0.0731607071326501\n",
      "train loss:0.06954518083189154\n",
      "train loss:0.02693339239563492\n",
      "train loss:0.03566498931708916\n",
      "train loss:0.08071988564312438\n",
      "train loss:0.01745028694496762\n",
      "train loss:0.03625840562914775\n",
      "train loss:0.016630976930847557\n",
      "train loss:0.03383967770312265\n",
      "train loss:0.02350021370721943\n",
      "train loss:0.03164826122589055\n",
      "train loss:0.02704345826173105\n",
      "train loss:0.028193004127265907\n",
      "train loss:0.024040666636979454\n",
      "train loss:0.020638935145304082\n",
      "train loss:0.045763371477643276\n",
      "train loss:0.057664754291928404\n",
      "train loss:0.026378379517954787\n",
      "train loss:0.06497509551198508\n",
      "train loss:0.02208040303281693\n",
      "train loss:0.04870220251121204\n",
      "train loss:0.029345193331651694\n",
      "train loss:0.03544382217422703\n",
      "train loss:0.08806230263932074\n",
      "train loss:0.058551357200131146\n",
      "train loss:0.02348545905462191\n",
      "train loss:0.00933155487441092\n",
      "train loss:0.027379305809707058\n",
      "train loss:0.03522803700373951\n",
      "train loss:0.02713951934510913\n",
      "train loss:0.025787645971401197\n",
      "=== epoch:15, train acc:0.981, test acc:0.964 ===\n",
      "train loss:0.0344237469742144\n",
      "train loss:0.10001524022223024\n",
      "train loss:0.062387873354880566\n",
      "train loss:0.072865166195857\n",
      "train loss:0.03584185554025249\n",
      "train loss:0.08414140063241737\n",
      "train loss:0.057850155172071344\n",
      "train loss:0.07210105187645298\n",
      "train loss:0.0245471211835632\n",
      "train loss:0.04524838164359392\n",
      "train loss:0.015480770529099077\n",
      "train loss:0.03916539001442692\n",
      "train loss:0.02187184966136288\n",
      "train loss:0.07965630425425908\n",
      "train loss:0.04779642018287901\n",
      "train loss:0.026675347895273947\n",
      "train loss:0.11928610953997008\n",
      "train loss:0.04428462558311647\n",
      "train loss:0.050966165116556404\n",
      "train loss:0.06267597154930377\n",
      "train loss:0.007044764285666122\n",
      "train loss:0.06764706340438915\n",
      "train loss:0.01362827770336736\n",
      "train loss:0.06252278474887715\n",
      "train loss:0.02777462211968499\n",
      "train loss:0.02029430396313117\n",
      "train loss:0.05014807347895246\n",
      "train loss:0.06255723240458802\n",
      "train loss:0.0234150677470212\n",
      "train loss:0.02443565973913719\n",
      "train loss:0.0438353217749391\n",
      "train loss:0.04274762652287985\n",
      "train loss:0.05605753057918053\n",
      "train loss:0.03998245193530792\n",
      "train loss:0.02520263612299529\n",
      "train loss:0.03187971585865599\n",
      "train loss:0.02890674574416073\n",
      "train loss:0.027590781848486325\n",
      "train loss:0.018218684836367852\n",
      "train loss:0.020248528982309097\n",
      "train loss:0.014010266902871344\n",
      "train loss:0.027002581184128623\n",
      "train loss:0.03314526402071118\n",
      "train loss:0.05018546735328892\n",
      "train loss:0.027144241436947734\n",
      "train loss:0.032414683496973434\n",
      "train loss:0.013062218507766682\n",
      "train loss:0.015469211220083358\n",
      "train loss:0.019060414927123067\n",
      "train loss:0.014925598674758955\n",
      "=== epoch:16, train acc:0.984, test acc:0.962 ===\n",
      "train loss:0.045751347075976605\n",
      "train loss:0.05228044141087586\n",
      "train loss:0.03963315407280589\n",
      "train loss:0.030440506052686778\n",
      "train loss:0.02837051124822466\n",
      "train loss:0.03042277008133817\n",
      "train loss:0.03852507072308256\n",
      "train loss:0.037889512868631084\n",
      "train loss:0.039971595428951276\n",
      "train loss:0.03489435508676817\n",
      "train loss:0.024623582803527347\n",
      "train loss:0.03727055489756351\n",
      "train loss:0.021180784012056014\n",
      "train loss:0.02020158780440621\n",
      "train loss:0.03406411114125294\n",
      "train loss:0.01654873343839626\n",
      "train loss:0.08124626408780891\n",
      "train loss:0.06831375649700883\n",
      "train loss:0.03783690508138858\n",
      "train loss:0.025678803043329097\n",
      "train loss:0.027881511444052823\n",
      "train loss:0.02825337047166099\n",
      "train loss:0.021384880313605605\n",
      "train loss:0.014686222244433429\n",
      "train loss:0.0413841538561817\n",
      "train loss:0.03527520450246628\n",
      "train loss:0.07708084450512112\n",
      "train loss:0.04407452454182815\n",
      "train loss:0.01637332161546847\n",
      "train loss:0.07369336066341296\n",
      "train loss:0.0636896947891596\n",
      "train loss:0.022061551145406567\n",
      "train loss:0.038832486052760753\n",
      "train loss:0.04548947099419427\n",
      "train loss:0.09747699452205656\n",
      "train loss:0.09720248784280021\n",
      "train loss:0.05030846465416319\n",
      "train loss:0.05068491373651303\n",
      "train loss:0.028423763895298753\n",
      "train loss:0.08908283792504296\n",
      "train loss:0.031920163071315684\n",
      "train loss:0.060490686903180164\n",
      "train loss:0.08568600603676042\n",
      "train loss:0.013922663947675482\n",
      "train loss:0.03047596999510149\n",
      "train loss:0.030512445230313597\n",
      "train loss:0.07542079433674327\n",
      "train loss:0.09161473069690654\n",
      "train loss:0.034723910824305204\n",
      "train loss:0.02355219121510952\n",
      "=== epoch:17, train acc:0.986, test acc:0.965 ===\n",
      "train loss:0.0335525280903956\n",
      "train loss:0.04036535805748902\n",
      "train loss:0.02483215805858904\n",
      "train loss:0.020385733970376196\n",
      "train loss:0.08819033830414863\n",
      "train loss:0.01978723020510418\n",
      "train loss:0.07001226792517683\n",
      "train loss:0.030014908845436106\n",
      "train loss:0.028415368233770017\n",
      "train loss:0.020233716232374084\n",
      "train loss:0.021386338452755282\n",
      "train loss:0.020248957496513263\n",
      "train loss:0.030940201511252347\n",
      "train loss:0.039087462390002735\n",
      "train loss:0.021254280903085006\n",
      "train loss:0.042958961514989846\n",
      "train loss:0.019709937916139775\n",
      "train loss:0.01107126903536316\n",
      "train loss:0.10752186479823823\n",
      "train loss:0.019325975845243967\n",
      "train loss:0.019350368839560584\n",
      "train loss:0.02148040393354162\n",
      "train loss:0.051794097056899045\n",
      "train loss:0.03122083244762069\n",
      "train loss:0.019510521706006114\n",
      "train loss:0.028752012156071774\n",
      "train loss:0.035250251404265215\n",
      "train loss:0.052605530823963555\n",
      "train loss:0.009372235692231527\n",
      "train loss:0.04113548914738489\n",
      "train loss:0.017586032327616144\n",
      "train loss:0.02916932570517529\n",
      "train loss:0.014542871507238177\n",
      "train loss:0.05227335171585423\n",
      "train loss:0.02520560429353873\n",
      "train loss:0.03341206283771079\n",
      "train loss:0.02359802885707355\n",
      "train loss:0.011718118448680084\n",
      "train loss:0.028388157627165317\n",
      "train loss:0.044664450447283954\n",
      "train loss:0.013705612717319815\n",
      "train loss:0.05838267550412493\n",
      "train loss:0.029404372800513365\n",
      "train loss:0.021575509777417373\n",
      "train loss:0.03116596807550335\n",
      "train loss:0.030184595370321386\n",
      "train loss:0.010927883462081957\n",
      "train loss:0.03780175897030988\n",
      "train loss:0.06241852294952481\n",
      "train loss:0.020975851956939077\n",
      "=== epoch:18, train acc:0.993, test acc:0.959 ===\n",
      "train loss:0.04252390896059708\n",
      "train loss:0.012660800368795737\n",
      "train loss:0.02541391961080949\n",
      "train loss:0.022484262263493378\n",
      "train loss:0.029135638624936056\n",
      "train loss:0.009119890481463706\n",
      "train loss:0.008744362432490804\n",
      "train loss:0.0588321359401952\n",
      "train loss:0.028697798190128198\n",
      "train loss:0.018453761978532105\n",
      "train loss:0.01774269141836217\n",
      "train loss:0.022185572859916746\n",
      "train loss:0.021181635520571566\n",
      "train loss:0.016213350998277136\n",
      "train loss:0.008455355866987603\n",
      "train loss:0.03284337484347968\n",
      "train loss:0.03161881037770574\n",
      "train loss:0.015032679065037398\n",
      "train loss:0.017573650029574093\n",
      "train loss:0.07950252509053785\n",
      "train loss:0.04840531432625121\n",
      "train loss:0.04191860545825574\n",
      "train loss:0.039746682630409376\n",
      "train loss:0.015174528327219277\n",
      "train loss:0.03413236237737108\n",
      "train loss:0.030724009087767398\n",
      "train loss:0.033170763464983766\n",
      "train loss:0.0382147317580271\n",
      "train loss:0.08021042607732413\n",
      "train loss:0.014955128166448528\n",
      "train loss:0.07531528305201549\n",
      "train loss:0.05411844746143573\n",
      "train loss:0.04064890579665445\n",
      "train loss:0.03221786056619123\n",
      "train loss:0.016055912647880158\n",
      "train loss:0.025495935739339403\n",
      "train loss:0.0480963695323334\n",
      "train loss:0.03289251947938424\n",
      "train loss:0.030557284795254783\n",
      "train loss:0.030301834184552373\n",
      "train loss:0.04939402053859458\n",
      "train loss:0.0336526588319247\n",
      "train loss:0.02301671396459232\n",
      "train loss:0.04068391240551846\n",
      "train loss:0.022471605209292936\n",
      "train loss:0.03534910760210528\n",
      "train loss:0.0157823595008021\n",
      "train loss:0.02081995720271438\n",
      "train loss:0.01974994045227975\n",
      "train loss:0.016283434534932736\n",
      "=== epoch:19, train acc:0.994, test acc:0.968 ===\n",
      "train loss:0.018662693298340777\n",
      "train loss:0.06907133068243991\n",
      "train loss:0.03456962441423942\n",
      "train loss:0.03855658409646992\n",
      "train loss:0.02015699784031341\n",
      "train loss:0.014820699038253232\n",
      "train loss:0.024482524581205783\n",
      "train loss:0.017783176434787776\n",
      "train loss:0.03311910363442205\n",
      "train loss:0.015757640869224883\n",
      "train loss:0.01103372008918996\n",
      "train loss:0.013512712091722064\n",
      "train loss:0.01280715910776162\n",
      "train loss:0.02395116239096067\n",
      "train loss:0.05671427647752826\n",
      "train loss:0.02644254329955961\n",
      "train loss:0.016208435202418995\n",
      "train loss:0.02248449020207296\n",
      "train loss:0.010823312234637545\n",
      "train loss:0.01812279210071368\n",
      "train loss:0.022797923623498306\n",
      "train loss:0.02229018852530808\n",
      "train loss:0.01738653117958997\n",
      "train loss:0.011314670426457502\n",
      "train loss:0.022817624312019603\n",
      "train loss:0.008969654175497445\n",
      "train loss:0.02610651697826973\n",
      "train loss:0.009583662034161144\n",
      "train loss:0.05540588275789048\n",
      "train loss:0.015567543718925379\n",
      "train loss:0.030298380953005178\n",
      "train loss:0.03881318676390121\n",
      "train loss:0.04132316476607236\n",
      "train loss:0.008117260189349199\n",
      "train loss:0.01103501729479324\n",
      "train loss:0.05580749643858949\n",
      "train loss:0.024032465006324243\n",
      "train loss:0.0058360134409803695\n",
      "train loss:0.022628495047486277\n",
      "train loss:0.02228040028644031\n",
      "train loss:0.02408389552844549\n",
      "train loss:0.044837080026064784\n",
      "train loss:0.012384657757097343\n",
      "train loss:0.018290243512321388\n",
      "train loss:0.03292209678188832\n",
      "train loss:0.014288682010851572\n",
      "train loss:0.026891640417855904\n",
      "train loss:0.009331014963906958\n",
      "train loss:0.010799791227411282\n",
      "train loss:0.013830499002295951\n",
      "=== epoch:20, train acc:0.993, test acc:0.959 ===\n",
      "train loss:0.015495714547489912\n",
      "train loss:0.027089364425671215\n",
      "train loss:0.01315146215876792\n",
      "train loss:0.031178909533242094\n",
      "train loss:0.023047647987256985\n",
      "train loss:0.025970554014974695\n",
      "train loss:0.026160408496367443\n",
      "train loss:0.01267477826724029\n",
      "train loss:0.005728116392295169\n",
      "train loss:0.026657884726910285\n",
      "train loss:0.015489718524780089\n",
      "train loss:0.01790561473962533\n",
      "train loss:0.03310756286731438\n",
      "train loss:0.013158815272622033\n",
      "train loss:0.015011372131282636\n",
      "train loss:0.01784762267187597\n",
      "train loss:0.02928295006244893\n",
      "train loss:0.025883544333474073\n",
      "train loss:0.022790123957216423\n",
      "train loss:0.021268108810740975\n",
      "train loss:0.010126225953916228\n",
      "train loss:0.05905248685947761\n",
      "train loss:0.03387545622019218\n",
      "train loss:0.01603476203922362\n",
      "train loss:0.02576170083412723\n",
      "train loss:0.012143663899391975\n",
      "train loss:0.031858318165454216\n",
      "train loss:0.031769562815914795\n",
      "train loss:0.01609829662934851\n",
      "train loss:0.025065243243884695\n",
      "train loss:0.023122139275238852\n",
      "train loss:0.017784204019256303\n",
      "train loss:0.025846807328841988\n",
      "train loss:0.006627375549115205\n",
      "train loss:0.005782043048720071\n",
      "train loss:0.00931964095908889\n",
      "train loss:0.026662148352096234\n",
      "train loss:0.025334573894637512\n",
      "train loss:0.015667460142516187\n",
      "train loss:0.07500885360055974\n",
      "train loss:0.022501380369699654\n",
      "train loss:0.020159103378260382\n",
      "train loss:0.02516794703626765\n",
      "train loss:0.0158616089852364\n",
      "train loss:0.00516238875699822\n",
      "train loss:0.018726659402652335\n",
      "train loss:0.024220051886272197\n",
      "train loss:0.01061246073087404\n",
      "train loss:0.02884048409512645\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.963\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSN0lEQVR4nO3deXhTZd4//vdJ0iRd032D0pbdWkQpiGyjohQRcdwG1FHA7TcoioALIDMKjI/ggooyoD6C6MgXGRV8cGSUOuyLbJa1FRAKLdBSSmm6b8n9+yNpaOiWZjtJ+n5dV64kJ+ecfA4Hrry57/vcRxJCCBARERH5CIXcBRARERE5E8MNERER+RSGGyIiIvIpDDdERETkUxhuiIiIyKcw3BAREZFPYbghIiIin8JwQ0RERD6F4YaIiIh8CsMNERER+RRZw83WrVsxZswYxMfHQ5IkfPfdd21us2XLFqSlpUGr1aJr16746KOPXF8oEREReQ1Zw01FRQX69u2LxYsX27R+Tk4O7rzzTgwbNgyZmZl45ZVXMGXKFHz77bcurpSIiIi8heQpN86UJAlr167FPffc0+I6M2bMwLp165CdnW1ZNmnSJBw8eBC7du1yQ5VERETk6VRyF9Aeu3btQnp6utWykSNHYtmyZairq4Ofn1+TbWpqalBTU2N5bzQaUVxcjIiICEiS5PKaiYiIyHFCCJSVlSE+Ph4KResdT14VbgoKChATE2O1LCYmBvX19SgqKkJcXFyTbebPn4+5c+e6q0QiIiJyoby8PHTu3LnVdbwq3ABo0trS0KvWUivMrFmzMH36dMt7vV6PLl26IC8vDyEhIa4rlIiIfE5GVgEW/Oc3XCi90iMQE6LBzFG9MSIl1qF919QbUFhagwv6alyqqEW90Yh6g4DBKFAvBIxGI+qN5vfmZ4MBMDQsF6blRqNAvdFoWs8gcKG0BrtOXWrz+wd3j0BciBZKhQSVQoJCoYBKKUEpmd4rGz0a3quUEhQKCSpJglKhsCz3VytxS69oh/48rlZaWoqEhAQEBwe3ua5XhZvY2FgUFBRYLSssLIRKpUJERESz22g0Gmg0mibLQ0JCGG6IiGRgMArsySlGYVk1ooO1uDE5HEqF5w8T+PFIPl787gQElFBoAizLi2qAF787gaVBwbgjtWkPAgBU1tYjX1+NAn21+bkKBaWN35sCjas0rrclv+RVAahyyvdFB2tw94DuTtnX1WwZUuJV4WbQoEH4/vvvrZZt2LAB/fv3b3a8DREReZYfj+Rj7vdZyNdXW5bF6bR4bUxKi8HAExiMAnO/z0JzV+A0LHtl7REUV9SisKzGKrTk66tQWl1v0/do/RSI0/kjKkgDtUrRtJXE0opibiVRNvpcYd160vD5ucuVWLk7r83vfmhAAjqF+TdpHTK1HhlhEI3fN/q8oZWo0fo6f3l/k2UNN+Xl5fj9998t73NycnDgwAGEh4ejS5cumDVrFs6dO4cvvvgCgOnKqMWLF2P69Ol46qmnsGvXLixbtgyrVq2S6xCIiMhGPx7Jx9Nf/tokIBToq/H0l79i6SP9nBZwhBCoMwjU1BtQU280PepaeF1vQE1do9f1RvN7g2VZXnGlVSBrTnFFLV5Ze6TFz4M1KsTqtKZHiBZxOi1idf7mZ9N7nb+f0y92MRgFNv52EQX66mbDmQQgVqfF6/f28YoWNFvIGm727duHW2+91fK+YWzMhAkTsGLFCuTn5yM3N9fyeXJyMtavX49p06bhH//4B+Lj4/HBBx/g/vvvd3vtRERy8raunbZaPiQAr607im5RQaipN6Kiph6VtQZU1JqeK2vqUVFrQGVtPSpqzM9XLa+sMa9vfjbKMNFJSlww+iaEWQWWOJ0WMSFaBGvlac1QKiS8NiYFT3/5KyTA6hw0/I15bUyKR//9aS+PmefGXUpLS6HT6aDX6znmhoi8kqd27RiNAqXVdSiuqMXlyjpcrqhFcWUtLlfU4sh5Pb4/mC9bbRqVwvTwU155rVJC49fodSufX9BXY9Xetrt2Vj11EwZ1a34MqNw89e+Nrdrz++1VY26IiJzF21o+Grija0eYr7qpqjOgpKLOElBMoeWq50afX66sdbi1ROunQKi/GgEaJQLVKgSolQjUmJ/VKgRolAhQKxGgViFQrUSARmVZfvX6/mpTUFErFQ539RiMApuPt921c2NyuEPf4zIlebgj/BJGjA/H0XOlKK6sRXiAGtd2CoFSugCU1AOhCXJX6TQMN0TU4Xjr/2ANRoE561of1PrSN4eQnV+GOoOt40mMqG3mM0dCSrBGhbBANcIC1QgP8ENYoBrVtQYcOHIEYVJZi9tdFsFYOPEuj2z5UCokzL8tFO+sNc2G31zXzou3DfLMgFySByxOA+proARwXXPrqDTAs/t9JuAw3BBRh+LOQa22MhoFSqrqcLGsxvQor77yuqwGF8tNz+dLqlBeY2h1X2XV9Vj03xNOqy1ArURYgBrhV4WV8AA1Qs3PYYF+CG9YFqCGWtV09ljD5VzU//4CNKhr8btq4AdV+K0APC/coCQPt2wYhVs0NS2vs0ED9PTAgFB5CahvpW7A9HnlJc+r3U4MN0TUYdgyqHXu91kYkRLb7v+BG1u4NLasur5RUKm2BJWi8lrL8qLyGtQ7cfTr4G4R6BUb3GgcSaMxJTaNOzG91pqfnUFZVQxlK8EGgCn4VBUDYV2c8p1O1QEDgjdjuCGiDmPXyaJWL+cVAPL11bht4WZo/ZTWYcUgms7/0SjEOOPSjLAAP0QFa0yPIM2V18EaRAVpcb6kCi9/e6jN/Tw3vIdHdu2QG9VWAJdPA8WngNPbbdvmxM+A0QCEJwMBHjp2yEYMN0RkN08dlKuvqsPJi+U4WViOU0UVOFlYjpMXy5FTVGHT9qcvVTqtlgC1EtFWIcU6tESa30cEaprtzmnMYBR47+fj3jmotd7GmW8PrQbO7gWUakClNY0FafW50WtlCz9pRqPp+2srgNpy8/PVr9v4rPyi7fUXnQCCooGgGNOzfxjgihs1V+uB4hxTgCk+deX15RygzI4r0zb93fQAAG0oEN71qkey6TkwyjXH40S8FJyI7CL3oFyjUeBcSRV+v1iOUxcrLGHm5MUKFJW30X3QhpmjeqNPJ5317LAKhWWW2KuXKxS48vlV995x9oRsm/fsb31Q672DcMuNaU79TpvUlAP6PKAkt/lHZZHra5CU1mFHGK6Ek2bjoJso/Mxhp1HgCYq56rX5WR14ZTshgKrLjcLLVSGmrT9T/zAgLNn0fPK/bdcZ2xeouAiUnW99PXXQlaBz9SMoFmjjjt32as/vN8MNEbVbS4NyG35gnTkot6KmHjlF5vDSKMTkFFWgpt7Y4naxIVp0jQpEt6ggdIsKRLfoICRFBOJPH+3ChdLWWz62zxjuES1QTTS66qVFrrrqpUl4OXNVeGn7xow26Z4OqANMx1hf3fazsfVxPE2og0wBwvIIauH1Ve/LC4EfZ7a9/263AYZa0/rlF4DqkvbXFxRt+s6SXFPrTGsCo5pvYQlr1LV0/gDwyc1tf/f/twWIvx6orbzSpXV1qNLnodWgqPIHwpKAiG7A2H86NehwnhsiAgDU1htx/EIZiitqTfN9NDeA1Pza1rlA2jMoVyEBNfVGlFbVobS6HqXVdVdeV9WZ37e+vLaVAKNWKpAUGWAOMEHoFm0KM8mRgS3OBrvgdi+9nBdwzqBWIYC6KtOPZpNHifX7qmJToCrJNb1ui1YHhHYBQhPNz10AXYLpuVoPfH5X2/sYPtv0A2sro6GF4FMFKFTWIUXlb/+P7fkDtq1326vW9dfXmFpDyi9cCTyW50avyy6Yu87KgeJy630Gx1t3CzU8hyUDWhf8J10dAMSkmB5Xq68BLp+50v3VOPxcPmM6hovZpuNwUQuOLRhuiHxEncGIExfKceScHofOleDwWT2y88tQa2g5HFytratp1ColKmrqbRqUe8O8DaiuM7br+1sSEahu1ApzJcR0DgtoXxDxhst5jUbAWH/Vw2B6Lr9g2z5+WWJqwWk2wOhN+7KHNvRKaAlNNP0ZNQ4x/qEtb2trOGgvhdL0Y6xu+67XslBpAF1n06M1QpgCQUPYqSkz/ZmGJTl+bAERpjraavELsGEQukoDRPU0Pa5mqDO17BSfMgVoGTHcEMnI3gG5BqPAyYvlOHRWj8NnS3DonB5Z50ub7aYJ0aoQH+rfaFI365sGNmZZZuMdjFvT+C7IkgSEaP0Q4q8yPTd6Hdx4ub8fQrQq87Npuc7fz3n35HGk5UMIU6tAS4GhpUddVfNBpaX3zhgbcmh12+tISlNLS4uPUNOzrrM5wCSY3tvLmT+wcnB1/ZIEaIJNj4hu9u2jJaEJpq7K1roOAyIcD/RKvytdYzJjuCGSia0Dco1GgVNFFTh8rsQcZvQ4er4UVXVNJ3ML1qiQ2kmH6zrr0KezDn066dAlPKDF7iYhBGobQk9dyzPYNg5D2fl6/Gf7vjZnmn3+/lsxtEcUQrSm6fEVHtHVY2Nw+HmO6cfm6qBiqHVpda2SlICksG2MyXXjgMgeVwKKJqRpeFEHuveKF3f9wLqKL9TvqbW5AAcUE8mgtQG5AsDjQ5KgkCQcOqfH0XN6VNQ2DTKBaiWu7aTDdZ2uBJmkiECXhwjD5VzUL+rX9kyzz/8KpasnY6urNv3YNH5UXW66rPISUFlsGvvgaECRFM2HhYYgcfXDT2u6WkahMj+UjV43976FdSSp/QNDiXwIBxQTebC2BuQCwPIdp62Wa/0USI2/EmKu66xDcmSQLANfnT7TbMNYg2o9UF3a/CDXyuKmQaXyElBn27w17TZ4ChBzbdOgogkxDU6VcaAkEbWN4YbIzfbkFLc6ILdBekoMRqTE4LrOoegWFQiV0st+UE9sAPJ2t3wljuVRapqPxF4Klak7oOHhH2b93vIINw3UXPVg2/tMvZ8tH0RejOGGyI0uV9Ri5e4zNq07+ro4/PH6Ti6uqJ2MRtNlwbbY9D/t27dC1ULXTkjLYSUgwtSaYuvYEVddseMu3j4ol8hNGG6I3ODEhTIs33EaazPPorrOiHgUtTkgNzpY68YKm2E0muaxOJ955ZF/0NSFZItO/U1X2tg8NsXf46d0l523D2olchOGGyIXMRoFthy/iOU7crDtxJVp0m+OqcIn+hfaHpAbfisAN/0PXAjTjKRWQeYQUNPM7KhKDWCw4fYGoxd6XteOL7R8dLCrXojswXBD5GQVNfX49tezWLHjNE6Zb9SokID0lFg8NiQJN2pzIX3ixAG57SWEaaItS5A5YHpubpp4pQaI7QPE32B+XG+6QunT4c6vyx3Y8kHUITDcEDlJXnElvth1Gl/tzUOZeQK7YI0K4wYkYMLgJCSEm2cZPZ/n2BcZjaYBuA2Tvhnq2p4YTn8WyD9wJdA09+Ou8ANiU68EmbjrgehrTBNzNebt41bY8kHk8xhuiBwghMDe05exfHsONmQVwGi+ljs5MhATByfh/rTOCNLY+c/siz+axqBcHVSE47czgEIFRKdYt8hEp5i6ZNriC107ROTTGG7I69l7CwNH1NQb8O+D+fhsZw6OnCu1LB/aPRKPD03CLT2jrSfTEwIoOg6c3gZk/9u2L2nv3YSlViaHCwg3BZi464H4fqY5XPzsHLDMrh0i8nAMN+TVbL2FgbNcLKvByt1n8OUvuSgqN7VcaFQK3NevEyYOTkav2GDTikIAhb+Zwszp7cCZHabZcdvjgRWmENKeGWzdhV07ROTBGG7Ia7V0C4MCfTWe/vJXLH2kn90Bx2AUpnsr1Znup5Svr8LK3blYd+C85S7XMSEajB+UhIdu7ILwAD/g4m/Anu3mQLMDqCyy3qlKCyTcCER0B/Ytb7uI8OTm77xLREStYrghr2TLLQxe+PogdvxehDqDMN/08UpYaesGkfXG5vZscn1CKB4fkohRMSXwy90I/LC9hTDjbwozScOApKFAp36msSjnD9gWboiIyC4MN+R1qmoN+GLXaUj6s7i2tYnwaoLxz18cmNbfTCEBQWoFHkyqwKNxeUjQ/wps2NF0zInKH+gy0BRkEhuFmatxQC4RkUsx3JDHqzcYcficHjt+L8L234vw65kSRBoKsVHzArRSy/PFVAs/zEn8HJ2TekKjUkLjp4BGpTC9VinM75WWZVpUIbDyPPwrzkFTfhZ+Feeg1OdBoc8Fik8BZ0qAxndO8Aswt8wMNbXOxPcDVOq2D4gDcomIXIrhhjyOEAInL1ZYwswvpy5Z5o1pkBxQBa2x9YnwtFIdHk4NwnU39jAtqCk3TV5Xkmt65J+58rokt/WwAZjDzMBGYeYG28JMczggl4jIZRhuyCNcKK22hJkdvxfhQql1l02IVoXB3SIxpEckhnaPREL1ceDTtvfbJ+sdIPNVU3ipKm57A60O0HUBQpt5RPW2P8wQEZHbMNyQLEqr67D7VDF2mMPMiULrmzGqVQoMSArDkO6mMHNtvM40d43RAJScAc7+YtP3SKe3Wi/QhppbTRKbhhddAuAf6pwDJCIi2TDckFvUGYzYf+ayJcwcPKuHodEVSZIE9Omks4SZtFgVtPpTQNF+4NhxYOcJoOgEcOl3wFBr+xcPmgIkDjIHmARTywwREfk0hhtyKSEE/nOkAG+sz8bZy1VWnyVHBGBUFwNuidAjVXsBAfpTwIXjwNETQNn5lneq0gIhnYDik20X0Od+z7szNRERuRTDDbnM0fN6zP0+C3tyihGKMvzJ/xhuCS9BquYC4upzob58CsiuaHkHgdFAZE8gsof52fxalwAUHAI+udl9B0NERF6D4Yacrqi8Bgs3HMdXe3MhBDBYdRyfat9HQH0JcPUFSQoVEN61aYiJ6M7xL0REZBeGG3Ka2nojPt95Gh/89wTKakyXbv896TAeubgQUn0tEJYMJA4BIrtfCTFhSYDSr/1fxonwiIioBQw35DAhBDb+VojXf8hGTpGpm6lPfBA+7vQj4g8vMa10zRjg3o8BdaBzvpQT4RERUQsYbsghvxeWYd6/s7H1uOmO15FBGsy6LQH3nfk7pMPfm1Ya9gJw618BhcK5X86J8IiIqBkMN2QXfWUd3vv5OP75yxkYjAJ+SgmPD03Gc/0DEbTmESD/IKBUA3d/CPR9UO5yiYioA2G4oXapNxixak8u3s04jsuVptsfjEiJwew7r0FS7XHgi3uBsnxTl9C4laY5ZoiIiNyI4YZstuP3Isz7PgvHLpjuxN0zJgiv3nUthvaIBLL+D1jzF6C+ynSbgodXmwYLExERuRnDDbXpzKUK/M8P2diQdQEAEBrghxdG9MRDN3aBSiEBW98GNr5uWrn77cADyzkTMBERyYbhhmAwCuzJKUZhWTWig7W4MTkcSoWEsuo6LN70Oz7bfhq1BiOUCgmP3pSIqbf3QGiA2nQZ9trngEOrTTsaOAlI/x9Ayb9WREQkH/4KdXA/HsnH3O+zkK+vtiyLDdFiREoM/nOkAEXlpnlkhvWIxKt3paBHTLBppfKLwOo/A3m7AUkJ3PkWMOBJOQ6BiIjICsNNB/bjkXw8/eWvEFctLyitxj9/OQMASI4MxF9HX4PhvaMhSZJphQtZwKpxQEkuoNEBY1cA3Ya7tXYiIqKWMNx0UAajwNzvs5oEm8ZCtCqsnzIM/mrllYUnMoCvHwNqy0wzDj/8LyCqp8vrJSIispWTZ1Ujb7Enp9iqK6o5pdX1OJBXYnojBPDLUuD/jTUFm8QhwFMbGWyIiMjjsOWmgyosaz3YWK1nqAP+8zKwb7lp4Q2PAKPfA1RqF1ZIRERkH4abDio6WGvTenHqGuDL+4GcLQAkYMQ8YPBzQMP4GyIiIg/DcNNBlVTWtvq5BKB/cDEG/PdPwKXfAb9A4P5Pgd53uqdAIiIiOzHcdED/2puHmWsOIR5FCJPKIAFWA4slAKmKU5gn/gXpUhkQ0hl4+Csgto9MFRMREdmO4aaD+WjLSSz4z2+IRxG2+L8IP9FKC049gJg+wCPfAsExbquRiIjIEbxaqoMQQuCN9dlY8J/fAABP9de1HmwajF7IYENERF6FLTcdQL3BiJlrDuOb/WcBALPvvAaPdS8FjtiwsUrj2uKIiIicjC03Pq66zoBJX/6Kb/afhVIh4e0HrsNTf+gqd1lEREQuw5YbH1ZaXYcnP9+HPTnF0KgUWPxwP4xIYRcTERH5NoYbH3WxrAYTlu9BVn4pgjUqfDqhPwZ2jTB9WPgb8J8Z8hZIRETkIgw3PiivuBKPLNuNM5cqERmkweePD8C18TrTnbw3zwf2rwCEQe4yiYiIXILhxsf8VlCK8cv2oLCsBgnh/vjyiYFIDFEC298Dti403RcKAJKGAae3yVssERGRCzDc+JC9p4vxxIq9KK2uR+/YYHzx2ABE5/4AfDEX0OeaVorrC4x8AwhNBBanAfU1Le9QpQECItxTPBERkZMw3PiIjb9dwDMrf0V1nRH9E8Ow4nYg6OsxwNm9phWC44HbXwP6jAUU5ovknt0PVF5qeacBEUBoguuLJyIiciKGGx+wNvMsXvz6EAxGgbHdDHgj5GOoVn5n+tAvEBg6DRg0GVAHWG8YmsDwQkREPofhxsst356Def/OQggqsCj+v7ilYA2kc7UAJOCGR4DhfwWCY+Uuk4iIyG0YbryUEAILNxzHR5t+w6PKjZilXYuAYr3pw663AOn/A8SmylojERGRHGSfoXjJkiVITk6GVqtFWloatm1r/QqelStXom/fvggICEBcXBwee+wxXLrUyrgRH2QwCsxeexhZW/6Fn9Qz8He/FQgw6IHIXsDDXwOPfsdgQ0REHZas4Wb16tWYOnUqZs+ejczMTAwbNgyjRo1Cbm5us+tv374d48ePxxNPPIGjR4/i66+/xt69e/Hkk0+6uXL51NQb8OaKf2F05iQsV7+Dbop808Df0QuBp3cCPdMBSZK7TCIiItlIQggh15cPHDgQ/fr1w9KlSy3LrrnmGtxzzz2YP39+k/XfeecdLF26FCdPnrQs+/DDD/HWW28hLy/Ppu8sLS2FTqeDXq9HSEiI4wfhRhVFedi7fDr+UJEBhSRgVPhBMWgyMGw6oNXJXR4REZHLtOf3W7aWm9raWuzfvx/p6elWy9PT07Fz585mtxk8eDDOnj2L9evXQwiBCxcu4JtvvsHo0aNb/J6amhqUlpZaPbxRVeFJ4B834pbKDVBIAhcT74Liuf3AiLkMNkRERI3IFm6KiopgMBgQE2N9I8eYmBgUFBQ0u83gwYOxcuVKjBs3Dmq1GrGxsQgNDcWHH37Y4vfMnz8fOp3O8khI8M5Ln49v+RcCRSXOIA6/370WUY+tBMIS5S6LiIjI48g+oFi6anyIEKLJsgZZWVmYMmUKXn31Vezfvx8//vgjcnJyMGnSpBb3P2vWLOj1esvD1u4rT2MoPg0AOB11K7r3Gy5vMURERB5MtkvBIyMjoVQqm7TSFBYWNmnNaTB//nwMGTIEL730EgDguuuuQ2BgIIYNG4bXX38dcXFxTbbRaDTQaDTOPwA305SbBlmLULbWEBERtUa2lhu1Wo20tDRkZGRYLc/IyMDgwYOb3aayshIKhXXJSqUSgKnFx5eFVJ0DAPhFJstcCRERkWeTtVtq+vTp+PTTT7F8+XJkZ2dj2rRpyM3NtXQzzZo1C+PHj7esP2bMGKxZswZLly7FqVOnsGPHDkyZMgU33ngj4uPj5ToM1xMCUfX5AICg2B4yF0NEROTZZJ2heNy4cbh06RLmzZuH/Px8pKamYv369UhMNHW95OfnW815M3HiRJSVlWHx4sV44YUXEBoaiuHDh+PNN9+U6xDcwlh2AVrUwiAkRHTqKnc5REREHk3WeW7k4I3z3BT/tg3hX92FsyISsa+egEop+zhwIiIit/KKeW7Idvr83wEAFxSxDDZERERt4C+lF6gpNM3IrNf68LgiIiIiJ2G48QLS5dMAgMpA75yAkIiIyJ0YbryAptw08SDnuCEiImobw40XsMxxE8UrpYiIiNrCcOPp6msQaigCAATFdpe5GCIiIs/HcOPpSvKggEC50CImppPc1RAREXk8hhsPV1lougw8T0QhPixA5mqIiIg8H8ONhys9fwIAUKCIRaBG1gmliYiIvALDjYervXgKAFCi4Rw3REREtmC48XTmOW6qgrrIWwcREZGXYLjxcJzjhoiIqH0YbjyZEAipPg8AUEcmy1wMERGRd2C48WRVl+FvrAAABMV2k7kYIiIi78Bw48ku5wAACkQY4iLDZC6GiIjIOzDceLD6S6Zwkyui0SnUX+ZqiIiIvAPDjQcrzzdN4HcOMYgIVMtcDRERkXdguPFgNQ1z3Gg7QaGQZK6GiIjIOzDceDCp5AwAoDqQ95QiIiKyFcONB7syx02SvIUQERF5EYYbT2WoQ1B1AQDAL7KrzMUQERF5D4YbT6U/CyUMqBZ+CI3uLHc1REREXoPhxlOZ7ymVJ6LRKSxQ3lqIiIi8CMONhxKXTYOJc0U0OoVxjhsiIiJbMdx4qOrCkwCAs4hCrE4rczVERETeg+HGQzXMcVPsFw+NSilzNURERN6D4cZDSSWnAQDVwQnyFkJERORlGG48lKbMNMeNkXPcEBERtQvDjSeq1kNbrwcAaCKTZS6GiIjIuzDceCLzlVJFIgRREREyF0NERORdGG48UeM5bkJ5GTgREVF7MNx4InO4yRXRiGe4ISIiaheGGw9Ud8l0GTgn8CMiImo/hhsPVHsxBwBQqIyFzt9P5mqIiIi8C8ONB7LMcRPURd5CiIiIvBDDjacxGqCpOGd6HZYoby1EREReiOHG05TlQ2msQ51QQhvRWe5qiIiIvA7DjacxXyl1VkQiPixY3lqIiIi8EMONp2k0x018KO8GTkRE1F4MN56m0Rw3nXkZOBERUbsx3HgYY/FpAJzAj4iIyF4MNx6m/pJpjpvziEF0MLuliIiI2ovhxsNI5m6pquDOUCokeYshIiLyQgw3nqS2An7VRQAAoy5J3lqIiIi8FMONJ7l8BgBQIgIRFhElczFERETeieHGkzS6UqoTBxMTERHZheHGk5SYWm7yRBTDDRERkZ0YbjyJZQK/GF4GTkREZCeGGw8iLpsuA88V0ejECfyIiIjswnDjQYyXTgMwT+CnY7ghIiKyB8ONpxACUslpAECZthP81Up56yEiIvJSDDeeovwCFIYaGIQEKTRB7mqIiIi8FsONpzDPcZOPCMSGBctcDBERkfdiuPEUDXPcGDmYmIiIyBEMN56i0QR+vAyciIjIfgw3noKzExMRETkFw42nsEzgx3BDRETkCIYbD8EJ/IiIiJyD4cYT1FUDZQUAgEJVLMIC/GQuiIiIyHsx3HgCfR4kCJQLLQJ00ZAkSe6KiIiIvBbDjSdoNN4mPixA3lqIiIi8HMONJ2h0pVRnjrchIiJyCMONJ+Bl4ERERE4je7hZsmQJkpOTodVqkZaWhm3btrW6fk1NDWbPno3ExERoNBp069YNy5cvd1O1LmLploriBH5EREQOUsn55atXr8bUqVOxZMkSDBkyBB9//DFGjRqFrKwsdOnSpdltxo4diwsXLmDZsmXo3r07CgsLUV9f7+bKncx8X6lcEY3RDDdEREQOkTXcvPvuu3jiiSfw5JNPAgDef/99/PTTT1i6dCnmz5/fZP0ff/wRW7ZswalTpxAeHg4ASEpKcmfJzicExOXTkGAeUMxwQ0RE5BDZuqVqa2uxf/9+pKenWy1PT0/Hzp07m91m3bp16N+/P9566y106tQJPXv2xIsvvoiqqqoWv6empgalpaVWD49SWQyptgwAcA5RiNVpZS6IiIjIu8nWclNUVASDwYCYmBir5TExMSgoKGh2m1OnTmH79u3QarVYu3YtioqK8Mwzz6C4uLjFcTfz58/H3LlznV6/05jH2+SLcISFhMBPKfswKCIiIq8m+y/p1RPWCSFanMTOaDRCkiSsXLkSN954I+688068++67WLFiRYutN7NmzYJer7c88vLynH4MDml82wV2SRERETlMtpabyMhIKJXKJq00hYWFTVpzGsTFxaFTp07Q6XSWZddccw2EEDh79ix69OjRZBuNRgONRuPc4p2pxDSY+CyvlCIiInIK2Vpu1Go10tLSkJGRYbU8IyMDgwcPbnabIUOG4Pz58ygvL7csO378OBQKBTp37uzSel2mYY4bI2+YSURE5AyydktNnz4dn376KZYvX47s7GxMmzYNubm5mDRpEgBTl9L48eMt6z/88MOIiIjAY489hqysLGzduhUvvfQSHn/8cfj7e2kw4AR+RERETiXrpeDjxo3DpUuXMG/ePOTn5yM1NRXr169HYmIiACA/Px+5ubmW9YOCgpCRkYHnnnsO/fv3R0REBMaOHYvXX39drkNwXKNwczfDDRERkcMkIYSQuwh3Ki0thU6ng16vR0hIiLzFGOqA16MBYcSA6n9g5bQ/omdMsLw1EREReaD2/H7LfrVUh6bPA4QR1cIPFxHKAcVEREROYFe42bx5s5PL6KDMt13IE9HQ+asRpJG1l5CIiMgn2BVu7rjjDnTr1g2vv/66580b400ajbdhqw0REZFz2BVuzp8/j+effx5r1qxBcnIyRo4ciX/961+ora11dn2+jVdKEREROZ1d4SY8PBxTpkzBr7/+in379qFXr16YPHky4uLiMGXKFBw8eNDZdfomc7jJE9HoFMp7ShERETmDwwOKr7/+esycOROTJ09GRUUFli9fjrS0NAwbNgxHjx51Ro2+q3HLDSfwIyIicgq7w01dXR2++eYb3HnnnUhMTMRPP/2ExYsX48KFC8jJyUFCQgL+9Kc/ObNW32NpueGtF4iIiJzFrstznnvuOaxatQoA8Mgjj+Ctt95Camqq5fPAwEAsWLAASUlJTinSJ1WVANUlABq6pRhuiIiInMGucJOVlYUPP/wQ999/P9RqdbPrxMfHY9OmTQ4V59PMN8wsEiGohJbdUkRERE5iV7j573//2/aOVSrcfPPN9uy+Y2g03katVCAy0IPvXE5ERORF7BpzM3/+fCxfvrzJ8uXLl+PNN990uKgOwWqOGy0UCkneeoiIiHyEXeHm448/Ru/evZssv/baa/HRRx85XFSHwAn8iIiIXMKucFNQUIC4uLgmy6OiopCfn+9wUR2C1Rw3DDdERETOYle4SUhIwI4dO5os37FjB+Lj4x0uqkNodF8pttwQERE5j10Dip988klMnToVdXV1GD58OADTIOOXX34ZL7zwglML9ElGA1CSCwDINUbjPl4pRURE5DR2hZuXX34ZxcXFeOaZZyz3k9JqtZgxYwZmzZrl1AJ9Uul5wFiHOqhQgHB2SxERETmRXeFGkiS8+eab+Nvf/obs7Gz4+/ujR48e0Gh4ObNNzONtzolIGKFguCEiInIiu8JNg6CgIAwYMMBZtXQc5nBzxhgNAIjjTTOJiIicxu5ws3fvXnz99dfIzc21dE01WLNmjcOF+bRG95SKCtZAo1LKWw8REZEPsetqqa+++gpDhgxBVlYW1q5di7q6OmRlZWHjxo3Q6XTOrtH3mG+9kMvLwImIiJzOrnDzxhtv4L333sO///1vqNVqLFq0CNnZ2Rg7diy6dOni7Bp9j2UCvxiGGyIiIiezK9ycPHkSo0ePBgBoNBpUVFRAkiRMmzYNn3zyiVML9EmNJ/DjZeBEREROZVe4CQ8PR1lZGQCgU6dOOHLkCACgpKQElZWVzqvOF9WUAxUXAZhvvaDjYGIiIiJnsmtA8bBhw5CRkYE+ffpg7NixeP7557Fx40ZkZGTgtttuc3aNvsU83qZMCkIZAtApLEDmgoiIiHyLXeFm8eLFqK6uBgDMmjULfn5+2L59O+677z787W9/c2qBPqfRbRcAIJ6XgRMRETlVu8NNfX09vv/+e4wcORIAoFAo8PLLL+Pll192enE+yTze5pQhCgDQOZQtN0RERM7U7jE3KpUKTz/9NGpqalxRj+9rNJg4SKNCiL9D8ygSERHRVewaUDxw4EBkZmY6u5aOwXIZeDTiQ7WQJEneeoiIiHyMXc0GzzzzDF544QWcPXsWaWlpCAwMtPr8uuuuc0pxPqlRuOEcN0RERM5nV7gZN24cAGDKlCmWZZIkQQgBSZJgMBicU52vEcJytVSeiMYwhhsiIiKnsyvc5OTkOLuOjqH8AlBfDSMUOC8iOIEfERGRC9gVbhITE51dR8dg7pIqUkahHip2SxEREbmAXeHmiy++aPXz8ePH21WMz2s03gYAww0REZEL2BVunn/+eav3dXV1qKyshFqtRkBAAMNNS8zh5mRdJAAgnuGGiIjI6ey6FPzy5ctWj/Lychw7dgxDhw7FqlWrnF2j7zCHmzPGaCgVEmJCODsxERGRs9kVbprTo0cPLFiwoEmrDjVimcAvCrEhWigVnOOGiIjI2ZwWbgBAqVTi/PnzztylbzHfVypXRPNKKSIiIhexa8zNunXrrN4LIZCfn4/FixdjyJAhTinM59RVA2Wm4JcronErx9sQERG5hF3h5p577rF6L0kSoqKiMHz4cCxcuNAZdfmeklwAQLUiAJcRzMHERERELmJXuDEajc6uw/eZx9sUKmMBSOyWIiIichGnjrmhVjS6GzjAy8CJiIhcxa5w88ADD2DBggVNlr/99tv405/+5HBRPskcbk6Y57jhBH5ERESuYVe42bJlC0aPHt1k+R133IGtW7c6XJRPMt8w82R9wwR+nOOGiIjIFewKN+Xl5VCr1U2W+/n5obS01OGifFKjbqnwQDUC1HYNdyIiIqI22BVuUlNTsXr16ibLv/rqK6SkpDhclM8Rwuq+Umy1ISIich27mg/+9re/4f7778fJkycxfPhwAMB///tfrFq1Cl9//bVTC/QJlZeA2nIISDgnInELx9sQERG5jF3h5u6778Z3332HN954A9988w38/f1x3XXX4eeff8bNN9/s7Bq9n7nVpswvEjXVanQKDZC3HiIiIh9m98CP0aNHNzuomJrRMMeNKg4ABxMTERG5kl1jbvbu3Yvdu3c3Wb57927s27fP4aJ8TsN4G6NpjpvOnMCPiIjIZewKN5MnT0ZeXl6T5efOncPkyZMdLsrnWOa4iQDACfyIiIhcya5wk5WVhX79+jVZfsMNNyArK8vhonyOOdxkV5vCDSfwIyIich27wo1Go8GFCxeaLM/Pz4dKxflbmrhsmsAvV0RD66dAeGDTOYKIiIjIOewKNyNGjMCsWbOg1+sty0pKSvDKK69gxIgRTivOJxjqgNKzAIA8EYX4UH9IkiRzUURERL7LrmaWhQsX4g9/+AMSExNxww03AAAOHDiAmJgY/POf/3RqgV5PnwcIIwwKDS4iFL3ZJUVERORSdoWbTp064dChQ1i5ciUOHjwIf39/PPbYY3jooYfg5+fn7Bq9m3m8TYk2HqiUON6GiIjIxeweIBMYGIihQ4eiS5cuqK2tBQD85z//AWCa5I/MzOHmgtI0xw3DDRERkWvZFW5OnTqFe++9F4cPH4YkSRBCWI0jMRgMTivQ6zW6pxTAy8CJiIhcza4Bxc8//zySk5Nx4cIFBAQE4MiRI9iyZQv69++PzZs3O7lEL2cON7/XhgMAOnECPyIiIpeyq+Vm165d2LhxI6KioqBQKKBUKjF06FDMnz8fU6ZMQWZmprPr9F7mcHOkyhxu2HJDRETkUna13BgMBgQFBQEAIiMjcf78eQBAYmIijh075rzqfIF5jptT9VGQJCBWx/tKERERuZJdLTepqak4dOgQunbtioEDB+Ktt96CWq3GJ598gq5duzq7Ru9VdRmoLgFgmuMmJkQLP6VdeZKIiIhsZFe4+etf/4qKigoAwOuvv4677roLw4YNQ0REBFavXu3UAr2audWmWhOJqmotUjjehoiIyOXsCjcjR460vO7atSuysrJQXFyMsLAwzr7bmHm8jV4TD4BXShEREbmD0/pIwsPD7Qo2S5YsQXJyMrRaLdLS0rBt2zabttuxYwdUKhWuv/76dn+n21jmuIkFwMHERERE7iDrAJDVq1dj6tSpmD17NjIzMzFs2DCMGjUKubm5rW6n1+sxfvx43HbbbW6q1E5XzXHTKZSDiYmIiFxN1nDz7rvv4oknnsCTTz6Ja665Bu+//z4SEhKwdOnSVrf7y1/+gocffhiDBg1yU6V2KjGNuTlRGwGAc9wQERG5g2zhpra2Fvv370d6errV8vT0dOzcubPF7T777DOcPHkSr732mk3fU1NTg9LSUquH25hbbg5XhgEAOoUGuO+7iYiIOijZwk1RUREMBgNiYmKslsfExKCgoKDZbU6cOIGZM2di5cqVUKlsGws9f/586HQ6yyMhIcHh2m1iNAAlpu617GpTy008u6WIiIhcTvZJV64ehHz1faoaGAwGPPzww5g7dy569uxp8/5nzZoFvV5veeTl5Tlcs01KzwHGehgValxAGEK0KgRrecd0IiIiV7P7ruCOioyMhFKpbNJKU1hY2KQ1BwDKysqwb98+ZGZm4tlnnwUAGI1GCCGgUqmwYcMGDB8+vMl2Go0GGo3GNQfRGnOXVHVgPIyVCl4GTkRE5Caytdyo1WqkpaUhIyPDanlGRgYGDx7cZP2QkBAcPnwYBw4csDwmTZqEXr164cCBAxg4cKC7SreNeQK/EvMcN505mJiIiMgtZGu5AYDp06fj0UcfRf/+/TFo0CB88sknyM3NxaRJkwCYupTOnTuHL774AgqFAqmpqVbbR0dHQ6vVNlnuESxz3MQB4AR+RERE7iJruBk3bhwuXbqEefPmIT8/H6mpqVi/fj0SExMBAPn5+W3OeeOxzOHmjDEKACfwIyIichdJCCHkLsKdSktLodPpoNfrERIS4rov+t/bgHP78HboX/GPghR8+NANGNM33nXfR0RE5MPa8/st+9VSPsvccnO0IhQAJ/AjIiJyF4YbV6gpByqLAACZ5aEAgM7sliIiInILhhtXMN92waANg14EQK1UIDJIhsvRiYiIOiCGG1cwd0lVBXYGAMSFaqFQtP+O6URERNR+DDeuYA43JZpOAIB4HbukiIiI3IXhxhUsc9zEAuBgYiIiIndiuHGFq+a44QR+RERE7sNw4wrmcHOiLhIAr5QiIiJyJ4YbZzMagRLTrMpHKsMAsOWGiIjInRhunK38AlBfDSEpcUAfCIBjboiIiNyJ4cbZzF1SxpDOKKszXf4dp9PKWBAREVHHwnDjbFfNcRMZpIHWTyljQURERB0Lw42zmcPNZY3pJpnskiIiInIvhhtna5jjRmGe4yaUXVJERETuxHDjbOb7Sp0R0QCATrxSioiIyK0YbpytYY6b2ggAvAyciIjI3RhunKmuCijLBwAcNs9xw5YbIiIi92K4cSbz5H1QB+OYXgWALTdERETuxnDjTA1z3IQmoqiiDgDQmVdLERERuRXDjTOZw01lUAIAIECthM7fT8aCiIiIOh6V3AV4vZI8oPKS6fXZfQCACqMa10o56BIcAEl/FghNkLFAIiKijkUSQgi5i3Cn0tJS6HQ66PV6hISEOLazkjxgcRpQX9PyOioN8Ox+BhwiIiIHtOf3m91Sjqi81HqwAUyfN7TsEBERkcsx3BAREZFPYbghIiIin8JwQ0RERD6F4YaIiIh8CsMNERER+RSGGyIiIvIpDDeOCIgwzWPTGpXGtB4RERG5BWcodkRogmmCvkbz2Hy9/yxW7DyN4b2i8UJ6T1Ow4QR+REREbsNw46jQBKvwcmiPCkeFhFviugHxvWUsjIiIqGNit5STnSupAgB0Cg2QuRIiIqKOieHGyc6bw018qFbmSoiIiDomhhsnO3fZFG46h/nLXAkREVHHxHDjRKXVdSirqQcAxIcy3BAREcmB4caJGlptwgL8EKDmWG0iIiI5MNw40ZXxNmy1ISIikgvDjRNduVKK4YaIiEguDDdOYjAK7M0pBgAoFKb3RERE5H4MN07w45F8DH1zI74/lG9+fwFD39yIH4/ky1wZERFRx8Nw46Afj+Tj6S9/Rb6+2mp5gb4aT3/5KwMOERGRmzHcOMBgFJj7fRaa64BqWDb3+yx2UREREbkRw40D9uQUN2mxaUwAyNdXY495LA4RERG5HsONAwrLWg429qxHREREjmO4cUB0sG33j7J1PSIiInIcw40DbkwOR5xOC6mFzyUAcTotbkwOd2dZREREHRrDjQOUCgmvjUkBgCYBp+H9a2NSoFS0FH+IiIjI2RhuHHRHahyWPtIPsTrrrqdYnRZLH+mHO1LjZKqMiIioY+LdHZ3gjtQ4jEiJxZ6cYhSWVSM62NQVxRYbIiIi92O4cRKlQsKgbhFyl0FERNThsVuKiIiIfArDDREREfkUhhsiIiLyKQw3RERE5FMYboiIiMinMNwQERGRT2G4ISIiIp/CcENEREQ+heGGiIiIfArDDREREfkUhhsiIiLyKQw3RERE5FNkDzdLlixBcnIytFot0tLSsG3bthbXXbNmDUaMGIGoqCiEhIRg0KBB+Omnn9xYLREREXk6WcPN6tWrMXXqVMyePRuZmZkYNmwYRo0ahdzc3GbX37p1K0aMGIH169dj//79uPXWWzFmzBhkZma6uXIiIiLyVJIQQsj15QMHDkS/fv2wdOlSy7JrrrkG99xzD+bPn2/TPq699lqMGzcOr776qk3rl5aWQqfTQa/XIyQkxK66iYiIyL3a8/stW8tNbW0t9u/fj/T0dKvl6enp2Llzp037MBqNKCsrQ3h4eIvr1NTUoLS01OpBREREvku2cFNUVASDwYCYmBir5TExMSgoKLBpHwsXLkRFRQXGjh3b4jrz58+HTqezPBISEhyqm4iIiDyb7AOKJUmyei+EaLKsOatWrcKcOXOwevVqREdHt7jerFmzoNfrLY+8vDyHayYiIiLPpZLriyMjI6FUKpu00hQWFjZpzbna6tWr8cQTT+Drr7/G7bff3uq6Go0GGo3G4XqJiIjIO8jWcqNWq5GWloaMjAyr5RkZGRg8eHCL261atQoTJ07E//t//w+jR492dZlERETkZWRruQGA6dOn49FHH0X//v0xaNAgfPLJJ8jNzcWkSZMAmLqUzp07hy+++AKAKdiMHz8eixYtwk033WRp9fH394dOp5PtOIiIiMhzyBpuxo0bh0uXLmHevHnIz89Hamoq1q9fj8TERABAfn6+1Zw3H3/8Merr6zF58mRMnjzZsnzChAlYsWKFu8snIiIiDyTrPDdy4Dw3RERE3scr5rkhIiIicgWGGyIiIvIpDDdERETkUxhuiIiIyKcw3BAREZFPYbghIiIin8JwQ0RERD6F4YaIiIh8CsMNERER+RSGGyIiIvIpDDdERETkUxhuiIiIyKcw3BAREZFPUcldABERkS8xGAyoq6uTuwyvpFaroVA43u7CcENEROQEQggUFBSgpKRE7lK8lkKhQHJyMtRqtUP7YbghIiJygoZgEx0djYCAAEiSJHdJXsVoNOL8+fPIz89Hly5dHPrzY7ghIiJykMFgsASbiIgIucvxWlFRUTh//jzq6+vh5+dn9344oJiIiMhBDWNsAgICZK7EuzV0RxkMBof2w3BDRETkJOyKcoyz/vwYboiIiMinMNwQERF5CINRYNfJS/i/A+ew6+QlGIxC7pLaJSkpCe+//77cZXBAMRERkSf48Ug+5n6fhXx9tWVZnE6L18ak4I7UOJd97y233ILrr7/eKaFk7969CAwMdLwoB7HlhoiISGY/HsnH01/+ahVsAKBAX42nv/wVPx7Jl6ky0/w99fX1Nq0bFRXlEYOqGW6IiIhcQAiBytr6Nh9l1XV4bd1RNNcB1bBszroslFXX2bQ/IWzvypo4cSK2bNmCRYsWQZIkSJKEFStWQJIk/PTTT+jfvz80Gg22bduGkydP4o9//CNiYmIQFBSEAQMG4Oeff7ba39XdUpIk4dNPP8W9996LgIAA9OjRA+vWrWv/H2Y7sVuKiIjIBarqDEh59SeH9yMAFJRWo8+cDTatnzVvJALUtv28L1q0CMePH0dqairmzZsHADh69CgA4OWXX8Y777yDrl27IjQ0FGfPnsWdd96J119/HVqtFp9//jnGjBmDY8eOoUuXLi1+x9y5c/HWW2/h7bffxocffog///nPOHPmDMLDw22q0R5suSEiIuqgdDod1Go1AgICEBsbi9jYWCiVSgDAvHnzMGLECHTr1g0RERHo27cv/vKXv6BPnz7o0aMHXn/9dXTt2rXNlpiJEyfioYceQvfu3fHGG2+goqICe/bscelxseWGiIjIBfz9lMiaN7LN9fbkFGPiZ3vbXG/FYwNwY3LbrR3+fkqb6mtL//79rd5XVFRg7ty5+Pe//22ZRbiqqgq5ubmt7ue6666zvA4MDERwcDAKCwudUmNLGG6IiIhcQJIkm7qHhvWIQpxOiwJ9dbPjbiQAsTothvWIglLhvkkCr77q6aWXXsJPP/2Ed955B927d4e/vz8eeOAB1NbWtrqfq2+jIEkSjEaj0+ttjN1SREREMlIqJLw2JgWAKcg01vD+tTEpLgs2arXaptsdbNu2DRMnTsS9996LPn36IDY2FqdPn3ZJTY5iuCEiIpLZHalxWPpIP8TqtFbLY3VaLH2kn0vnuUlKSsLu3btx+vRpFBUVtdiq0r17d6xZswYHDhzAwYMH8fDDD7u8BcZe7JYiIiLyAHekxmFESiz25BSjsKwa0cFa3Jgc7vKuqBdffBETJkxASkoKqqqq8NlnnzW73nvvvYfHH38cgwcPRmRkJGbMmIHS0lKX1mYvSbTngngfUFpaCp1OB71ej5CQELnLISIiH1BdXY2cnBwkJydDq9W2vQE1q7U/x/b8frNbioiIiHwKww0RERH5FIYbIiIi8ikMN0RERORTGG6IiIjIpzDcEBERkU9huCEiIiKfwnBDREREPoXhhoiIiHwKb79AREQkt5I8oPJSy58HRAChCe6rx8sx3BAREcmpJA9YnAbU17S8jkoDPLvfJQHnlltuwfXXX4/333/fKfubOHEiSkpK8N133zllf/ZgtxQREZGcKi+1HmwA0+etteyQFYYbIiIiVxACqK1o+1FfZdv+6qts21877oc9ceJEbNmyBYsWLYIkSZAkCadPn0ZWVhbuvPNOBAUFISYmBo8++iiKioos233zzTfo06cP/P39ERERgdtvvx0VFRWYM2cOPv/8c/zf//2fZX+bN29u5x+c49gtRURE5Ap1lcAb8c7b3/I7bFvvlfOAOtCmVRctWoTjx48jNTUV8+bNAwAYDAbcfPPNeOqpp/Duu++iqqoKM2bMwNixY7Fx40bk5+fjoYcewltvvYV7770XZWVl2LZtG4QQePHFF5GdnY3S0lJ89tlnAIDw8HC7DtcRDDdEREQdlE6ng1qtRkBAAGJjYwEAr776Kvr164c33njDst7y5cuRkJCA48ePo7y8HPX19bjvvvuQmJgIAOjTp49lXX9/f9TU1Fj2JweGGyIiIlfwCzC1orSl4JBtrTKP/wjEXmfb9zpg//792LRpE4KCgpp8dvLkSaSnp+O2225Dnz59MHLkSKSnp+OBBx5AWFiYQ9/rTAw3REREriBJtnUPqfxt25/K3+buJkcYjUaMGTMGb775ZpPP4uLioFQqkZGRgZ07d2LDhg348MMPMXv2bOzevRvJyckur88WHFBMRETUganVahgMBsv7fv364ejRo0hKSkL37t2tHoGBpnAlSRKGDBmCuXPnIjMzE2q1GmvXrm12f3JguCEiIpJTQIRpHpvWqDSm9VwgKSkJu3fvxunTp1FUVITJkyejuLgYDz30EPbs2YNTp05hw4YNePzxx2EwGLB792688cYb2LdvH3Jzc7FmzRpcvHgR11xzjWV/hw4dwrFjx1BUVIS6ujqX1N0adksRERHJKTTBNEGfTDMUv/jii5gwYQJSUlJQVVWFnJwc7NixAzNmzMDIkSNRU1ODxMRE3HHHHVAoFAgJCcHWrVvx/vvvo7S0FImJiVi4cCFGjRoFAHjqqaewefNm9O/fH+Xl5di0aRNuueUWl9TeEkmIdlwQ7wNKS0uh0+mg1+sREhIidzlEROQDqqurkZOTg+TkZGi1WrnL8Vqt/Tm25/eb3VJERETkUxhuiIiIyKcw3BAREZFPYbghIiIin8JwQ0RE5CQd7Bodp3PWnx/DDRERkYP8/PwAAJWVlTJX4t1qa2sBAEql0qH9cJ4bIiIiBymVSoSGhqKwsBAAEBAQAEmSZK7KuxiNRly8eBEBAQFQqRyLJww3RERETtBwF+yGgEPtp1Ao0KVLF4eDIcMNERGRE0iShLi4OERHR8tyywFfoFaroVA4PmKG4YaIiMiJlEqlw2NGyDGyDyhesmSJZZrltLQ0bNu2rdX1t2zZgrS0NGi1WnTt2hUfffSRmyolIiIibyBruFm9ejWmTp2K2bNnIzMzE8OGDcOoUaOQm5vb7Po5OTm48847MWzYMGRmZuKVV17BlClT8O2337q5ciIiIvJUst44c+DAgejXrx+WLl1qWXbNNdfgnnvuwfz585usP2PGDKxbtw7Z2dmWZZMmTcLBgwexa9cum76TN84kIiLyPu35/ZZtzE1tbS3279+PmTNnWi1PT0/Hzp07m91m165dSE9Pt1o2cuRILFu2DHV1dZZ5BhqrqalBTU2N5b1erwdg+kMiIiIi79Dwu21Lm4xs4aaoqAgGgwExMTFWy2NiYlBQUNDsNgUFBc2uX19fj6KiIsTFxTXZZv78+Zg7d26T5QkJCQ5UT0RERHIoKyuDTqdrdR3Zr5a6+lp2IUSr17c3t35zyxvMmjUL06dPt7w3Go0oLi5GRESE0ydYKi0tRUJCAvLy8ny+y6sjHSvQsY6Xx+q7OtLx8lh9jxACZWVliI+Pb3Nd2cJNZGQklEplk1aawsLCJq0zDWJjY5tdX6VSISIiotltNBoNNBqN1bLQ0FD7C7dBSEiIT/8Fa6wjHSvQsY6Xx+q7OtLx8lh9S1stNg1ku1pKrVYjLS0NGRkZVsszMjIwePDgZrcZNGhQk/U3bNiA/v37NzvehoiIiDoeWS8Fnz59Oj799FMsX74c2dnZmDZtGnJzczFp0iQApi6l8ePHW9afNGkSzpw5g+nTpyM7OxvLly/HsmXL8OKLL8p1CERERORhZB1zM27cOFy6dAnz5s1Dfn4+UlNTsX79eiQmJgIA8vPzrea8SU5Oxvr16zFt2jT84x//QHx8PD744APcf//9ch2CFY1Gg9dee61JN5gv6kjHCnSs4+Wx+q6OdLw81o5N1nluiIiIiJxN9tsvEBERETkTww0RERH5FIYbIiIi8ikMN0RERORTGG7aacmSJUhOToZWq0VaWhq2bdvW6vpbtmxBWloatFotunbtio8++shNldpv/vz5GDBgAIKDgxEdHY177rkHx44da3WbzZs3Q5KkJo/ffvvNTVXbb86cOU3qjo2NbXUbbzyvAJCUlNTseZo8eXKz63vTed26dSvGjBmD+Ph4SJKE7777zupzIQTmzJmD+Ph4+Pv745ZbbsHRo0fb3O+3336LlJQUaDQapKSkYO3atS46gvZp7Xjr6uowY8YM9OnTB4GBgYiPj8f48eNx/vz5Vve5YsWKZs93dXW1i4+mdW2d24kTJzap+aabbmpzv554bts61ubOjyRJePvtt1vcp6eeV1diuGmH1atXY+rUqZg9ezYyMzMxbNgwjBo1yupy9cZycnJw5513YtiwYcjMzMQrr7yCKVOm4Ntvv3Vz5e2zZcsWTJ48Gb/88gsyMjJQX1+P9PR0VFRUtLntsWPHkJ+fb3n06NHDDRU77tprr7Wq+/Dhwy2u663nFQD27t1rdZwNk2L+6U9/anU7bzivFRUV6Nu3LxYvXtzs52+99RbeffddLF68GHv37kVsbCxGjBiBsrKyFve5a9cujBs3Do8++igOHjyIRx99FGPHjsXu3btddRg2a+14Kysr8euvv+Jvf/sbfv31V6xZswbHjx/H3Xff3eZ+Q0JCrM51fn4+tFqtKw7BZm2dWwC44447rGpev359q/v01HPb1rFefW6WL18OSZLanBLFE8+rSwmy2Y033igmTZpktax3795i5syZza7/8ssvi969e1st+8tf/iJuuukml9XoCoWFhQKA2LJlS4vrbNq0SQAQly9fdl9hTvLaa6+Jvn372ry+r5xXIYR4/vnnRbdu3YTRaGz2c289rwDE2rVrLe+NRqOIjY0VCxYssCyrrq4WOp1OfPTRRy3uZ+zYseKOO+6wWjZy5Ejx4IMPOr1mR1x9vM3Zs2ePACDOnDnT4jqfffaZ0Ol0zi3OyZo71gkTJog//vGP7dqPN5xbW87rH//4RzF8+PBW1/GG8+psbLmxUW1tLfbv34/09HSr5enp6di5c2ez2+zatavJ+iNHjsS+fftQV1fnslqdTa/XAwDCw8PbXPeGG25AXFwcbrvtNmzatMnVpTnNiRMnEB8fj+TkZDz44IM4depUi+v6ynmtra3Fl19+iccff7zNm8h663ltkJOTg4KCAqvzptFocPPNN7f47xdo+Vy3to2n0uv1kCSpzXvrlZeXIzExEZ07d8Zdd92FzMxM9xTooM2bNyM6Oho9e/bEU089hcLCwlbX94Vze+HCBfzwww944okn2lzXW8+rvRhubFRUVASDwdDkpp4xMTFNbubZoKCgoNn16+vrUVRU5LJanUkIgenTp2Po0KFITU1tcb24uDh88skn+Pbbb7FmzRr06tULt912G7Zu3erGau0zcOBAfPHFF/jpp5/wv//7vygoKMDgwYNx6dKlZtf3hfMKAN999x1KSkowceLEFtfx5vPaWMO/0fb8+23Yrr3beKLq6mrMnDkTDz/8cKs3VuzduzdWrFiBdevWYdWqVdBqtRgyZAhOnDjhxmrbb9SoUVi5ciU2btyIhQsXYu/evRg+fDhqampa3MYXzu3nn3+O4OBg3Hfffa2u563n1RGy3n7BG139P1whRKv/621u/eaWe6pnn30Whw4dwvbt21tdr1evXujVq5fl/aBBg5CXl4d33nkHf/jDH1xdpkNGjRpled2nTx8MGjQI3bp1w+eff47p06c3u423n1cAWLZsGUaNGoX4+PgW1/Hm89qc9v77tXcbT1JXV4cHH3wQRqMRS5YsaXXdm266yWog7pAhQ9CvXz98+OGH+OCDD1xdqt3GjRtneZ2amor+/fsjMTERP/zwQ6s//N5+bpcvX44///nPbY6d8dbz6gi23NgoMjISSqWySaovLCxskv4bxMbGNru+SqVCRESEy2p1lueeew7r1q3Dpk2b0Llz53Zvf9NNN3nl/wwCAwPRp0+fFmv39vMKAGfOnMHPP/+MJ598st3beuN5bbj6rT3/fhu2a+82nqSurg5jx45FTk4OMjIyWm21aY5CocCAAQO87nzHxcUhMTGx1bq9/dxu27YNx44ds+vfsLee1/ZguLGRWq1GWlqa5eqSBhkZGRg8eHCz2wwaNKjJ+hs2bED//v3h5+fnslodJYTAs88+izVr1mDjxo1ITk62az+ZmZmIi4tzcnWuV1NTg+zs7BZr99bz2thnn32G6OhojB49ut3beuN5TU5ORmxsrNV5q62txZYtW1r89wu0fK5b28ZTNASbEydO4Oeff7YreAshcODAAa8735cuXUJeXl6rdXvzuQVMLa9paWno27dvu7f11vPaLnKNZPZGX331lfDz8xPLli0TWVlZYurUqSIwMFCcPn1aCCHEzJkzxaOPPmpZ/9SpUyIgIEBMmzZNZGVliWXLlgk/Pz/xzTffyHUINnn66aeFTqcTmzdvFvn5+ZZHZWWlZZ2rj/W9994Ta9euFcePHxdHjhwRM2fOFADEt99+K8chtMsLL7wgNm/eLE6dOiV++eUXcdddd4ng4GCfO68NDAaD6NKli5gxY0aTz7z5vJaVlYnMzEyRmZkpAIh3331XZGZmWq4OWrBggdDpdGLNmjXi8OHD4qGHHhJxcXGitLTUso9HH33U6urHHTt2CKVSKRYsWCCys7PFggULhEqlEr/88ovbj+9qrR1vXV2duPvuu0Xnzp3FgQMHrP4d19TUWPZx9fHOmTNH/Pjjj+LkyZMiMzNTPPbYY0KlUondu3fLcYgWrR1rWVmZeOGFF8TOnTtFTk6O2LRpkxg0aJDo1KmTV57btv4eCyGEXq8XAQEBYunSpc3uw1vOqysx3LTTP/7xD5GYmCjUarXo16+f1eXREyZMEDfffLPV+ps3bxY33HCDUKvVIikpqcW/jJ4EQLOPzz77zLLO1cf65ptvim7dugmtVivCwsLE0KFDxQ8//OD+4u0wbtw4ERcXJ/z8/ER8fLy47777xNGjRy2f+8p5bfDTTz8JAOLYsWNNPvPm89pw2frVjwkTJgghTJeDv/baayI2NlZoNBrxhz/8QRw+fNhqHzfffLNl/QZff/216NWrl/Dz8xO9e/f2mGDX2vHm5OS0+O9406ZNln1cfbxTp04VXbp0EWq1WkRFRYn09HSxc+dO9x/cVVo71srKSpGeni6ioqKEn5+f6NKli5gwYYLIzc212oe3nNu2/h4LIcTHH38s/P39RUlJSbP78Jbz6kqSEOaRkEREREQ+gGNuiIiIyKcw3BAREZFPYbghIiIin8JwQ0RERD6F4YaIiIh8CsMNERER+RSGGyIiIvIpDDdE1OFs3rwZkiShpKRE7lKIyAUYboiIiMinMNwQERGRT2G4ISK3E0LgrbfeQteuXeHv74++ffvim2++AXCly+iHH35A3759odVqMXDgQBw+fNhqH99++y2uvfZaaDQaJCUlYeHChVaf19TU4OWXX0ZCQgI0Gg169OiBZcuWWa2zf/9+9O/fHwEBARg8eDCOHTtm+ezgwYO49dZbERwcjJCQEKSlpWHfvn0u+hMhImdSyV0AEXU8f/3rX7FmzRosXboUPXr0wNatW/HII48gKirKss5LL72ERYsWITY2Fq+88gruvvtuHD9+HH5+fti/fz/Gjh2LOXPmYNy4cdi5cyeeeeYZREREYOLEiQCA8ePHY9euXfjggw/Qt29f5OTkoKioyKqO2bNnY+HChYiKisKkSZPw+OOPY8eOHQCAP//5z7jhhhuwdOlSKJVKHDhwAH5+fm77MyIiB8h8404i6mDKy8uFVqttclfiJ554Qjz00EOWuyJ/9dVXls8uXbok/P39xerVq4UQQjz88MNixIgRVtu/9NJLIiUlRQghxLFjxwQAkZGR0WwNDd/x888/W5b98MMPAoCoqqoSQggRHBwsVqxY4fgBE5HbsVuKiNwqKysL1dXVGDFiBIKCgiyPL774AidPnrSsN2jQIMvr8PBw9OrVC9nZ2QCA7OxsDBkyxGq/Q4YMwYkTJ2AwGHDgwAEolUrcfPPNrdZy3XXXWV7HxcUBAAoLCwEA06dPx5NPPonbb78dCxYssKqNiDwbww0RuZXRaAQA/PDDDzhw4IDlkZWVZRl30xJJkgCYxuw0vG4ghLC89vf3t6mWxt1MDftrqG/OnDk4evQoRo8ejY0bNyIlJQVr1661ab9EJC+GGyJyq5SUFGg0GuTm5qJ79+5Wj4SEBMt6v/zyi+X15cuXcfz4cfTu3duyj+3bt1vtd+fOnejZsyeUSiX69OkDo9GILVu2OFRrz549MW3aNGzYsAH33XcfPvvsM4f2R0TuwQHFRORWwcHBePHFFzFt2jQYjUYMHToUpaWl2LlzJ4KCgpCYmAgAmDdvHiIiIhATE4PZs2cjMjIS99xzDwDghRdewIABA/D3v/8d48aNw65du7B48WIsWbIEAJCUlIQJEybg8ccftwwoPnPmDAoLCzF27Ng2a6yqqsJLL72EBx54AMnJyTh79iz27t2L+++/32V/LkTkRHIP+iGijsdoNIpFixaJXr16CT8/PxEVFSVGjhwptmzZYhns+/3334trr71WqNVqMWDAAHHgwAGrfXzzzTciJSVF+Pn5iS5duoi3337b6vOqqioxbdo0ERcXJ9RqtejevbtYvny5EOLKgOLLly9b1s/MzBQARE5OjqipqREPPvigSEhIEGq1WsTHx4tnn33WMtiYiDybJESjjmoiIplt3rwZt956Ky5fvozQ0FC5yyEiL8QxN0RERORTGG6IiIjIp7BbioiIiHwKW26IiIjIpzDcEBERkU9huCEiIiKfwnBDREREPoXhhoiIiHwKww0RERH5FIYbIiIi8ikMN0RERORTGG6IiIjIp/z/jb36sWudFEoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47b33d-389b-4b10-a22d-a93e1ab018bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
